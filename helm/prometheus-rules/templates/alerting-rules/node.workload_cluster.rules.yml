apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    {{- include "labels.common" . | nindent 4 }}
{{- if not .Values.mimir.enabled }}
    cluster_type: "workload_cluster"
{{- end }}
  name: node.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: node
    rules:
    - alert: WorkloadClusterNodeIsUnschedulable
      annotations:
        description: '{{`Node {{ $labels.node }} is unschedulable.`}}'
        opsrecipe: node-is-unschedulable/
      expr: kube_node_spec_unschedulable != 0
      for: 45m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: kubernetes
    {{- if eq .Values.managementCluster.provider.kind "aws" }}
    - alert: AWSWorkloadClusterNodeTooManyAutoTermination
      annotations:
        description: '{{`Cluster {{ $labels.cluster_id }} has too many nodes terminated by node auto termination feature in a short time.`}}'
        opsrecipe: node-too-many-auto-termination-aws/
      expr: increase(aws_operator_unhealthy_node_termination_count[60m]) > 10
      for: 15m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        severity: page
        team: phoenix
        topic: kubernetes
    {{- end }}
    - alert: NodeStateFlappingUnderLoad
      # Check if the kubelet status is flapping, unless the node is under load.
      # It helps to read this rule from the bottom upwards.
      #
      # If the kubelet status is flapping between Ready and something else,
      # matching against the kubelet labels, as kube_node_status_condition labels
      # by the node name, and node_exporter labels by the node name,
      # unless,
      # the load over 15 minutes divided by number of CPUs is higher than 2 (the node is overloaded).
      annotations:
        description: '{{`Node {{ $labels.node }} status is flapping under load.`}}'
      expr: |
        (
          sum(node_load15{cluster_type="workload_cluster"})
            by (cluster_id, installation, node, pipeline, provider)
          / count(rate(node_cpu_seconds_total{cluster_type="workload_cluster"}[5m]))
            by (cluster_id, installation, node, pipeline, provider)
        ) >= 2
        unless on (cluster_id, installation, node, pipeline, provider) (
          kube_node_labels{cluster_type="workload_cluster"}
          and on (cluster_id, installation, node, pipeline, provider)
          changes(kube_node_status_condition{cluster_type="workload_cluster", condition="Ready", status="true"}[30m])
        ) >= 6
      for: 10m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: kubernetes
    - alert: NodeHasConstantOOMKills
      # Check if a core component is restarted because memory reasons
      # in the last hour.
      annotations:
        description: '{{`Node {{ $labels.ip }} has constant OOM kills.`}}'
      expr: kube_pod_container_status_restarts_total{cluster_type="workload_cluster", namespace=~"(giantswarm|kube-system)"} - kube_pod_container_status_restarts_total{cluster_type="workload_cluster"} offset 1h >= 1 AND ignoring(reason) kube_pod_container_status_last_terminated_reason{cluster_type="workload_cluster", reason="OOMKilled"} > 0
      for: 10m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: kubernetes
    - alert: NodeConnTrackAlmostExhausted
      annotations:
        description: '{{`Node {{ $labels.node }} reports a connection usage above 85% for the last 15 minutes.`}}'
        opsrecipe: node-conntrack-limits/
      expr: node_nf_conntrack_entries{cluster_type="workload_cluster"} / node_nf_conntrack_entries_limit{cluster_type="workload_cluster"} >= 0.85
      for: 15m
      labels:
        area: kaas
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        severity: page
        team: {{ include "providerTeam" . }}
        topic: kubernetes
    - alert: MachineEntropyTooLow
      annotations:
        description: '{{`Machine {{ $labels.instance }} entropy is too low.`}}'
        opsrecipe: low-entropy/
      expr: node_entropy_available_bits{cluster_type="workload_cluster"} < 250
      for: 10m
      labels:
        area: kaas
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: infrastructure
    - alert: MachineAllocatedFileDescriptorsTooHigh
      annotations:
        description: '{{`Machine {{ $labels.instance }} has too many allocated file descriptors.`}}'
        opsrecipe: high-number-file-descriptors/
      expr: node_filefd_allocated{cluster_type="workload_cluster"} / node_filefd_maximum{cluster_type="workload_cluster"} * 100 > 80
      for: 15m
      labels:
        area: kaas
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: infrastructure
    {{- if eq .Values.managementCluster.provider.kind "aws" }}
    - alert: WorkloadClusterNodeUnexpectedTaintNodeWithImpairedVolumes
      annotations:
        description: '{{`Node {{ $labels.node }} has unexpected taint NodeWithImpairedVolumes`}}'
        opsrecipe: aws-node-taint-NodeWithImpairedVolumes/
      expr: kube_node_spec_taint{key="NodeWithImpairedVolumes"}
      for: 30m
      labels:
        area: kaas
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: kubernetes
    {{- end }}
    - alert: WorkloadClusterMasterMemoryUsageTooHigh
      annotations:
        description: '{{`Machine {{ $labels.instance }} memory usage is too high (less than 10% and 2G of allocatable memory).`}}'
        opsrecipe: master-machine-usage-too-high/
      expr: |-
        ( ( (
              ( node_memory_MemTotal_bytes{cluster_type="workload_cluster"}
              - node_memory_MemFree_bytes{cluster_type="workload_cluster"}
              - node_memory_Cached_bytes{cluster_type="workload_cluster"}
              )
              / (node_memory_MemTotal_bytes{cluster_type="workload_cluster"}
            ) * 100
          )
        ) > 80
        and
        ( node_memory_MemFree_bytes{cluster_type="workload_cluster"}
          + node_memory_Cached_bytes{cluster_type="workload_cluster"}
        ) < 2147483648) 
        and on (cluster_id, node) kube_node_role{cluster_type="workload_cluster", role=~"control-plane|master"}
      for: 60m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        severity: page
        team: se
        topic: infrastructure
