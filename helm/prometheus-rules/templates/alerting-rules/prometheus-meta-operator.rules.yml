apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: prometheus-meta-operator
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: observability
    rules:
    - alert: "Heartbeat"
      expr: up{app="prometheus",instance!="prometheus-agent"}
      labels:
        area: "empowerment"
        installation: {{ .Values.managementCluster.name }}
        team: "atlas"
        topic: "observability"
        type: "heartbeat"
        namespace: "monitoring" # Needed due to https://github.com/prometheus-operator/prometheus-operator/issues/3737
      annotations:
        description: This alert is used to ensure the entire alerting pipeline is functional.
    - alert: "MatchingNumberOfPrometheusAndCluster"
      annotations:
        description: This alert is used to ensure we have as many workload cluster prometheus as we have workload cluster CR.
        opsrecipe: matching-number-of-prometheus-and-cluster/
      # This expression list all the cluster IDs that exist and are not being deleted and compares them (using unless) to the running prometheus pods.
      # If a prometheus is missing, this alert will fire. This alert will not check if a prometheus is running when it should not (e.g. deleted cluster)
      expr: |
        (
          sum by(cluster_id) (
            {__name__=~"cluster_service_cluster_info|cluster_operator_cluster_status", status!="Deleting"}
          ) unless sum by(cluster_id) (
            label_replace(
              kube_pod_container_status_running{container="prometheus", namespace!="{{ .Values.managementCluster.name }}-prometheus", namespace=~".*-prometheus"},
              "cluster_id", "$2", "pod", "(prometheus-)(.+)(-.+)"
            )
          )
        ) + (
          sum by (cluster_name) (
            capi_cluster_status_phase{phase!="Deleting"}
          ) unless sum by (cluster_name) (
            label_replace(kube_pod_container_status_running{container="prometheus",namespace=~".*-prometheus"}, 
            "cluster_name", "$2", "pod", "(prometheus-)(.+)(-.+)"
            )
          )
        )
        > 0
      for: 10m
      labels:
        area: "empowerment"
        cancel_if_mc_kube_state_metrics_down: "true"
        cancel_if_cluster_status_creating: "true"
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        installation: {{ .Values.managementCluster.name }}
        severity: "page"
        team: "atlas"
        topic: "observability"
    - alert: "PrometheusMetaOperatorReconcileErrors"
      annotations:
        description: '{{`prometheus-meta-operator controller {{ $labels.controller }} too many reconcile errors.`}}'
        opsrecipe: "pmo-reconcile-errors/"
        dashboard: piJK9Vm4z/operatorkit
      expr: |
        avg_over_time(operatorkit_controller_errors_total{app="prometheus-meta-operator"}[20m]) > 0
      for: 1h
      labels:
        area: "empowerment"
        cancel_if_mc_kube_state_metrics_down: "false"
        cancel_if_cluster_status_creating: "true"
        cancel_if_outside_working_hours: "true"
        installation: {{ .Values.managementCluster.name }}
        severity: "page"
        team: "atlas"
        topic: "observability"
