{{- if eq .Values.managementCluster.provider.flavor "vintage" }}
## TODO Remove when all vintage installations are gone
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    {{- include "labels.common" . | nindent 4 }}
    # No need for .Values.mimir.enabled condition - will be gone with Vintage
    cluster_type: "workload_cluster"
  name: aws.node.workload-cluster.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: aws.node
    rules:
    - alert: AWSWorkloadClusterNodeTooManyAutoTermination
      annotations:
        description: '{{`Cluster {{ $labels.cluster_id }} has too many nodes terminated by node auto termination feature in a short time.`}}'
        opsrecipe: node-too-many-auto-termination-aws/
      expr: increase(aws_operator_unhealthy_node_termination_count[60m]) > 10
      for: 15m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        severity: page
        team: phoenix
        topic: kubernetes
    - alert: WorkloadClusterNodeUnexpectedTaintNodeWithImpairedVolumes
      annotations:
        description: '{{`Node {{ $labels.node }} has unexpected taint NodeWithImpairedVolumes`}}'
        opsrecipe: aws-node-taint-NodeWithImpairedVolumes/
      expr: kube_node_spec_taint{key="NodeWithImpairedVolumes"}
      for: 30m
      labels:
        area: kaas
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: kubernetes
{{- end }}
