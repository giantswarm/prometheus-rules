{{- if eq .Values.managementCluster.provider.flavor "vintage" }}
## TODO Remove when all vintage installations are gone
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    {{- include "labels.common" . | nindent 4 }}
    # No need for .Values.mimir.enabled condition - will be gone with Vintage
    cluster_type: "management_cluster"
  name: aws.management-cluster.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: aws.management-cluster
    rules:
    - alert: AWSClusterCreationFailed
      annotations:
        description: '{{`Cluster {{ $labels.cluster_id }} creation is taking longer than expected.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/cluster-creation-failed/
      expr: (statusresource_cluster_status{pod=~"aws-operator.*", status="Creating"} == 1 or cluster_operator_cluster_status{pod=~"cluster-operator.*", status="Creating", provider="aws"} == 1)
      for: 30m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        cancel_if_stack_failed: "true"
        severity: page
        team: phoenix
        topic: aws
    - alert: AWSClusterUpdateFailed
      annotations:
        description: '{{`Cluster {{ $labels.cluster_id }} update is taking longer than expected.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/cluster-update-failed/
      expr: (statusresource_cluster_status{pod=~"aws-operator.*", status="Updating"} == 1 or cluster_operator_cluster_status{pod=~"cluster-operator.*", status="Updating", provider="aws"} == 1)
      for: 3h
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        cancel_if_stack_failed: "true"
        severity: page
        team: phoenix
        topic: aws
    - alert: AWSClusterDeletionFailed
      annotations:
        description: '{{`Cluster {{ $labels.cluster_id }} deletion is taking longer than expected.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/cluster-deletion-failed/
      expr: (statusresource_cluster_status{pod=~"aws-operator.*", status="Deleting"} == 1 or cluster_operator_cluster_status{pod=~"cluster-operator.*", status="Deleting", provider="aws"} == 1)
      for: 1h
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        severity: page
        team: phoenix
        topic: aws
    - alert: CloudFormationStackFailed
      annotations:
        description: '{{`CloudFormation Stack "{{ $labels.stack_type }}" for {{ $labels.installation }}/{{ $labels.cluster_id }}, id "{{ $labels.id }}" remains in {{$labels.state }} state.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/aws-cloudformation-failed-state/
      expr: aws_operator_cloudformation_info{state=~"CREATE_FAILED|DELETE_FAILED|ROLLBACK_FAILED|UPDATE_ROLLBACK_FAILED"}
      for: 10m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        stack_failed: "true"
        severity: page
        team: phoenix
        topic: aws
    - alert: CloudFormationStackRollback
      annotations:
        description: '{{`Creation or update of CloudFormation Stack "{{ $labels.stack_type }}" for {{ $labels.installation }}/{{ $labels.cluster_id }}, id "{{ $labels.id }}" is rolled back and remains in {{ $labels.state }} state.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/aws-cloudformation-rollback/
      expr: aws_operator_cloudformation_info{state=~"ROLLBACK_COMPLETE|ROLLBACK_IN_PROGRESS|UPDATE_ROLLBACK_IN_PROGRESS|UPDATE_ROLLBACK_COMPLETE|UPDATE_ROLLBACK_COMPLETE_CLEANUP_IN_PROGRESS"}
      for: 10m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        stack_failed: "true"
        severity: page
        team: phoenix
        topic: aws
    - alert: ELBHostsOutOfService
      annotations:
        description: '{{`ELB {{ $labels.elb }} has unhealthy hosts.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/elb-has-unhealthy-hosts/
      expr: aws_operator_elb_instance_out_of_service_count{elb=~"([a-z0-9]*)-(api|ingress)"} > 0
      for: 10m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        severity: page
        team: phoenix
        topic: aws
    - alert: ManagementClusterPodPendingAWS
      annotations:
        description: '{{`Pod {{ $labels.namespace }}/{{ $labels.pod }} is stuck in Pending.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/pod-stuck-in-pending/
      expr: kube_pod_status_phase{namespace="giantswarm",pod=~"(aws-operator.*|cluster-operator.*)",phase="Pending"} == 1
      for: 25m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_kube_state_metrics_down: "true"
        severity: page
        team: phoenix
        topic: managementcluster
    - alert: NATGatewaysPerVPCApproachingLimit
      annotations:
        description: '{{`AWS number of NAT Gateways for {{ $labels.vpc }} in account {{ $labels.account_id }} is too close to limit.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/service-usage-approaching-limit/
      expr: sum by (vpc, availability_zone, account_id) (aws_operator_nat_info) / ignoring(vpc, availability_zone) group_left count by (account_id) (aws_operator_servicequota_info) * 100 > 80
      for: 10m
      labels:
        area: kaas
        severity: notify
        team: phoenix
        topic: aws
    - alert: ServiceUsageApproachingLimit
      annotations:
        description: '{{`AWS service usage for {{ $labels.service }} - {{ $labels.name }} ({{ $labels.account_id}}/{{ $labels.region }}) is too close to limit.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/service-usage-approaching-limit/
      expr: aws_operator_service_usage / aws_operator_service_limit * 100 > 80
      for: 10m
      labels:
        area: kaas
        severity: notify
        team: phoenix
        topic: aws
    - alert: ManagementClusterContainerIsRestartingTooFrequentlyAWS
      annotations:
        description: '{{`Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting too often.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/container-is-restarting-too-often/
      ## route53-manager is only used in China as route53 did not used to exist there
      expr: label_join(increase(kube_pod_container_status_restarts_total{container=~"aws-admission-controller.*|aws-node.*|aws-operator.*|cluster-operator.*|route53-manager.*"}[1h]), "service", "/", "namespace", "pod") > 6
      for: 5m
      labels:
        area: kaas
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: phoenix
        topic: kubernetes
    - alert: ManagementClusterDeploymentMissingAWS
      annotations:
        description: '{{`Deployment {{ $labels.deployment }} is missing.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/management-cluster-deployment-is-missing/
      expr: absent(kube_deployment_status_condition{namespace="giantswarm", condition="Available", deployment="aws-admission-controller"})
      for: 15m
      labels:
        area: kaas
        cancel_if_monitoring_agent_down: "true"
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_kube_state_metrics_down: "true"
        severity: page
        team: phoenix
        topic: kubernetes
  - name: aws-jobs
    rules:
    - alert: JobHasNotBeenScheduledForTooLong
      annotations:
        description: '{{`CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} has not been scheduled for more than 2 hours.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/job-has-not-been-scheduled-for-too-long/
      expr: (time() - kube_cronjob_status_last_schedule_time{cronjob="route53-manager"}) > 7200
      for: 15m
      labels:
        area: kaas
        severity: page
        team: phoenix
        topic: managementcluster
{{- end }}
