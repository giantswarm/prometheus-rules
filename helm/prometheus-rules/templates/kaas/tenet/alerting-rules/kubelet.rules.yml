apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: kubelet.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: cadvisor
    rules:
      - alert: CadvisorDown
        annotations:
          description: '{{`Cadvisor ({{ $labels.instance }}) is down.`}}'
          runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/kubelet-is-down/
        expr: label_replace(up{job="kubelet", metrics_path="/metrics/cadvisor"}, "ip", "$1", "instance", "(.+):\\d+") == 0
        for: 1h
        labels:
          area: kaas
          cancel_if_kubelet_down: "true"
          cancel_if_cluster_has_no_workers: "true"
          cancel_if_outside_working_hours: "true"
          cancel_if_monitoring_agent_down: "true"
          severity: notify
          team: tenet
          topic: kubernetes
  - name: kubelet
    rules:
    - alert: KubeletConditionBad
      annotations:
        description: '{{`Kubelet {{ $labels.node }} has status condition {{ $labels.condition }}.`}}'
      expr: kube_node_status_condition{condition=~"DiskPressure|MemoryPressure|OutOfDisk", status="true"} == 1
      for: 10m
      labels:
        area: kaas
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        cancel_if_kubelet_down: "true"
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: kubernetes
    - alert: KubeletDockerOperationsErrorsTooHigh
      annotations:
        description: '{{`Kubelet ({{ $labels.instance }}) is reporting errors rates while performing runtime {{ $labels.operation_type }} operation.`}}'
      expr: rate(kubelet_runtime_operations_errors_total[5m]) > 1
      for: 10m
      labels:
        area: kaas
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        cancel_if_kubelet_down: "true"
        severity: notify
        team: tenet
        topic: kubernetes
    - alert: KubeletPLEGLatencyTooHigh
      annotations:
        description: '{{`Kubelet ({{ $labels.instance }}) PLEG latency is too high.`}}'
      expr: histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (cluster_id, installation, pipeline, provider, instance, le)) > 100
      for: 10m
      labels:
        area: kaas
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        cancel_if_kubelet_down: "true"
        severity: notify
        team: tenet
        topic: kubernetes
