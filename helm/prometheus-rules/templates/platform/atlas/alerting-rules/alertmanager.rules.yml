apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: alertmanager.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: alertmanager
    rules:
    - alert: AlertmanagerNotifyNotificationsFailing
      annotations:
        __dashboardUid__: alertmanager-overview
        dashboardQueryParams: "orgId=1"
        description: '{{`AlertManager {{ $labels.integration }} notifications are failing.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/alert-manager-notifications-failing/
      # Interval = 20m because currently AlertManager config set `group_interval=15m` that means that if a notification fails, it will be retried after 15m
      # so the counter will stay flat during this time.
      # Here, we decide to page after 3 successive failures, so we need to wait 3*15m = 45m before paging.
      expr: rate(alertmanager_notifications_failed_total{integration!="opsgenie", cluster_type="management_cluster"}[20m]) > 0
      for: 45m
      labels:
        area: platform
        severity: page
        team: atlas
        topic: monitoring
        cancel_if_outside_working_hours: "true"
    - alert: AlertmanagerPageNotificationsFailing
      annotations:
        __dashboardUid__: alertmanager-overview
        dashboardQueryParams: "orgId=1"
        description: '{{`AlertManager {{ $labels.integration }} notifications are failing.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/alert-manager-notifications-failing/
      # Here, we decide to notify after 2 successive failures (opsgenie notification), so we need to wait 2*15m = 30m before notifying.
      expr: rate(alertmanager_notifications_failed_total{integration="opsgenie", cluster_type="management_cluster"}[20m]) > 0
      for: 30m
      labels:
        area: platform
        severity: notify
        team: atlas
        topic: monitoring
