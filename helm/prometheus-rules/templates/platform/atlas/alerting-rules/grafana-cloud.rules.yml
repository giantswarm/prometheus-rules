apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
    {{- if eq .Values.managementCluster.provider.flavor "vintage" }}
    cluster_type: "management_cluster"
    {{- end }}
  name: grafana-cloud.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: grafana-cloud
    rules:
    ## Pages Atlas when prometheus fails to send samples to cortex
    - alert: PrometheusMissingGrafanaCloud
      annotations:
        description: 'Prometheus is not sending data to Grafana Cloud.'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/prometheus-grafanacloud/
      {{- if eq .Values.managementCluster.provider.flavor "capi" }}
      expr: absent(prometheus_remote_storage_samples_total{remote_name="grafana-cloud", cluster_type="management_cluster", cluster_id="{{ .Values.managementCluster.name }}", installation="{{ .Values.managementCluster.name }}", provider="{{ .Values.managementCluster.provider.kind }}", pipeline="{{ .Values.managementCluster.pipeline }}"})
      {{- else }}
      expr: absent(prometheus_remote_storage_samples_total{remote_name="grafana-cloud"})
      {{- end }}
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  - name: resource-usage
    rules:
    - alert: IncorrectResourceUsageData
      annotations:
        description: '{{`Data used in the Grafana Cloud Resource Usage dashboard is incorrect for cluster {{ $labels.cluster_id }}.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/resource-usage-dashboard/
      expr: |
        quantile_over_time(0.9, aggregation:node:cpu_cores_total[120m:30m])  / on(cluster_id, cluster_type, customer, installation, pipeline, provider, region)  quantile_over_time(0.9, (sum(machine_cpu_cores)by(cluster_id, cluster_type, customer, installation, pipeline, provider, region))[120m:30m]) < 0.9
        or
        quantile_over_time(0.9, aggregation:node:memory_memtotal_bytes_total[120m:30m])  / on(cluster_id, cluster_type, customer, installation, pipeline, provider, region)  quantile_over_time(0.9, (sum(machine_memory_bytes)by(cluster_id, cluster_type, customer, installation, pipeline, provider, region))[120m:30m]) < 0.9
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  {{- if eq .Values.managementCluster.provider.flavor "capi" }}
  - name: mimir-to-grafana-cloud-exporter
    rules:
    - alert: MimirToGrafanaCloudExporterDown
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is down.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
        __dashboardUid__: iWowmlSmk
        dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
      # We can use absent function here because the prometheus mimir-to-grafana-cloud is a MC component only
      expr: up{job="mimir/mimir-to-grafana-cloud"} == 0 or absent(up{job="mimir/mimir-to-grafana-cloud", cluster_type="management_cluster", cluster_id="{{ .Values.managementCluster.name }}", installation="{{ .Values.managementCluster.name }}", provider="{{ .Values.managementCluster.provider.kind }}", pipeline="{{ .Values.managementCluster.pipeline }}"})
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterFailures
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is failing to read or write data.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
        __dashboardUid__: promRW001
        dashboardQueryParams: "orgId=1"
      # We can have encountered failures with remote read and/or remote write
      # For remote read, we are looking the number of read queries are increasing
      # For remote write, we are looking the rate (on 10 minutes) of failed samples are not greater than 0 for 30 minutes
      expr: |
        (
          sum by (cluster_id, installation, provider, pipeline) (
            rate(prometheus_remote_storage_read_queries_total{job="mimir/mimir-to-grafana-cloud", code=~"2.."}[10m])
          ) == 0
        )
        or
        (
          sum by (cluster_id, installation, provider, pipeline) (
            rate(prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud"}[10m])
          ) > 0
        )
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterTooManyRestarts
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is restarting too much.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
        __dashboardUid__: promRW001
        dashboardQueryParams: "orgId=1"
      expr: |
        count by (pod, cluster_id, installation, provider, pipeline) (changes(kube_pod_status_ready{condition="true", namespace="mimir", pod=~"prometheus-mimir-to-grafana-cloud-.*"}[20m])) > 3
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  {{- end }}
