apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
    {{- if not .Values.mimir.enabled }}
    cluster_type: "management_cluster"
    {{- end }}
  name: grafana-cloud.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: grafana-cloud
    rules:
    ## Pages Atlas when prometheus fails to send samples to cortex
    - alert: PrometheusMissingGrafanaCloud
      annotations:
        description: 'Prometheus is not sending data to Grafana Cloud.'
        opsrecipe: prometheus-grafanacloud/
      {{- if .Values.mimir.enabled }}
      expr: absent(prometheus_remote_storage_samples_total{remote_name="grafana-cloud", cluster_type="management_cluster", cluster_id="{{ .Values.managementCluster.name }}", installation="{{ .Values.managementCluster.name }}", provider="{{ .Values.managementCluster.provider.kind }}", pipeline="{{ .Values.managementCluster.pipeline }}"})
      {{- else }}
      expr: absent(prometheus_remote_storage_samples_total{remote_name="grafana-cloud"})
      {{- end }}
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  {{- if .Values.mimir.enabled }}
  - name: mimir-to-grafana-cloud-exporter
    rules:
    - alert: MimirToGrafanaCloudExporterDown
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is down.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: iWowmlSmk/prometheus?var-cluster=mimir-to-grafana-cloud
      # We can use absent function here because the prometheus mimir-to-grafana-cloud is a MC component only
      expr: up{job="mimir/mimir-to-grafana-cloud"} == 0 or absent(up{job="mimir/mimir-to-grafana-cloud", cluster_type="management_cluster", cluster_id="{{ .Values.managementCluster.name }}", installation="{{ .Values.managementCluster.name }}", provider="{{ .Values.managementCluster.provider.kind }}", pipeline="{{ .Values.managementCluster.pipeline }}"})
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterFailures
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is failing to read or write data.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: promRW001/prometheus-remote-write
      # We can have encountered failures with remote read and/or remote write
      # For remote read, we are looking the number of read queries are increasing
      # For remote write, we are looking the rate (on 10 minutes) of failed samples are not greater than 0 for 30 minutes
      expr: |
        (
          sum by (cluster_id, installation, provider, pipeline) (
            rate(prometheus_remote_storage_read_queries_total{job="mimir/mimir-to-grafana-cloud", code=~"2.."}[10m])
          ) == 0
        )
        or
        (
          sum by (cluster_id, installation, provider, pipeline) (
            rate(prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud"}[10m])
          ) > 0
        )
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterTooManyRestarts
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is restarting too much.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: promRW001/prometheus-remote-write
      expr: |
        count by (pod, cluster_id, installation, provider, pipeline) (changes(kube_pod_status_ready{condition="true", namespace="mimir", pod=~"prometheus-mimir-to-grafana-cloud-.*"}[20m])) > 3
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  {{- end }}
