{{- if .Values.mimir.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: mimir-to-grafana-cloud-exporter.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: mimir-to-grafana-cloud-exporter
    rules:
    - alert: MimirToGrafanaCloudExporterDown
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is down.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: iWowmlSmk/prometheus?var-cluster=mimir-to-grafana-cloud
      expr: up{job="mimir/mimir-to-grafana-cloud"} == 0
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterFailures
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is failing to read or write data.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: promRW001/prometheus-remote-write
      # We can have encountered failures with remote read and/or remote write
      # For remote write, some errors can increased the number of samples failed (non-recoverable errors) and/or dropped (unknown errors)
      # See https://grafana.com/docs/agent/latest/flow/reference/components/prometheus.remote_write/#debug-metrics
      expr: |
        (
          rate(prometheus_remote_storage_read_queries_total{job="mimir/mimir-to-grafana-cloud"}[10m]) == 0
          or rate(prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud"}[10m]) > 0
          or rate(prometheus_remote_storage_samples_dropped_total{job="mimir/mimir-to-grafana-cloud"}[10m]) > 0
        )
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterTooManyRestarts
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is restarting too much.`}}'
        opsrecipe: mimir-grafana-cloud-exporter/
        dashboard: promRW001/prometheus-remote-write
      expr: |
        count by (pod) (changes(kube_pod_status_ready{condition="true", namespace="mimir", pod=~"prometheus-mimir-to-grafana-cloud-.*"}[20m])) > 3
      for: 20m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
{{- end }}
