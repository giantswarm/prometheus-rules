{{- if .Values.mimir.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: mimir-to-grafana-cloud-exporter.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: mimir-to-grafana-cloud-exporter
    rules:
    - alert: MimirToGrafanaCloudExporterDown
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is down.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: iWowmlSmk/prometheus?var-cluster=mimir-to-grafana-cloud
      # We can use absent function here because the prometheus mimir-to-grafana-cloud is a MC component only
      expr: up{job="mimir/mimir-to-grafana-cloud"} == 0 or absent(up{job="mimir/mimir-to-grafana-cloud"})
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterFailures
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is failing to read or write data.`}}'
        opsrecipe: mimir-grafana-cloud-exporter-failing/
        dashboard: promRW001/prometheus-remote-write
      # We can have encountered failures with remote read and/or remote write
      # For remote read, we are looking the number of read queries are increasing
      # For remote write, we are looking the rate (on 10 minutes) of failed samples are not greater than 0 for 30 minutes
      expr: |
        sum by (cluster_id, installation, provider, pipeline) (
          rate(prometheus_remote_storage_read_queries_total{job="mimir/mimir-to-grafana-cloud"}[10m]) == 0
          or rate(prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud"}[10m]) > 0
        )
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirToGrafanaCloudExporterTooManyRestarts
      annotations:
        description: '{{`Prometheus Mimir to Grafana-Cloud is restarting too much.`}}'
        opsrecipe: mimir-grafana-cloud-exporter/
        dashboard: promRW001/prometheus-remote-write
      expr: |
        count by (pod) (changes(kube_pod_status_ready{condition="true", namespace="mimir", pod=~"prometheus-mimir-to-grafana-cloud-.*"}[20m])) > 3
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
{{- end }}
