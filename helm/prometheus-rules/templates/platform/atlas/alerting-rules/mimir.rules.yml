{{- if .Values.mimir.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: mimir.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: mimir
    rules:
    # This alert is meant to always fire, to ensure the entire alerting pipeline is functional.
    - alert: "Heartbeat"
      annotations:
        description: This alert is used to ensure the entire alerting pipeline is functional.
        opsrecipe: mimir/
      expr: up{job=~"mimir/.*", container!="prometheus"} > 0
      labels:
        area: platform
        installation: {{ .Values.managementCluster.name }}
        # TODO(@team-atlas): We need this label as long as we have the old and new heartbeats. Let's remove once the legacy monitoring is gone
        type: mimir-heartbeat
        team: atlas
        topic: observability
    # Coming from https://github.com/giantswarm/giantswarm/issues/30124
    # This alert ensures Mimir containers are not restarting too often (flappiness).
    # If it is not the the case, this can incur high costs by cloud providers (s3 api calls are quite expensive).
    # This alert will not page for the prometheus-buddy.
    - alert: MimirRestartingTooOften
      annotations:
        description: '{{`Mimir containers are restarting too often.`}}'
        opsrecipe: mimir/
      expr: |
        increase(
          kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container!="prometheus"}[1h]
        ) > 5
      for: 5m
      labels:
        area: platform
        # This label is used to ensure the alert go through even for non-stable installations
        all_pipelines: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirComponentDown
      annotations:
        description: '{{`Mimir component : {{ $labels.service }} is down.`}}'
        opsrecipe: mimir/
      expr: count(up{job=~"mimir/.*", container!="prometheus"} == 0) by (cluster_id, installation, provider, pipeline, service) > 0
      for: 5m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: GrafanaAgentForPrometheusRulesDown
      annotations:
        description: 'Grafana-agent sending PrometheusRules to Mimir ruler is down.'
        opsrecipe: mimir/
      expr: count(up{job="grafana-agent-rules", namespace="mimir"} == 0) by (cluster_id, installation, provider, pipeline) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirRulerEventsFailed
      annotations:
        description: 'Mimir ruler is failing to process PrometheusRules.'
        opsrecipe: mimir/
      expr: rate(mimir_rules_events_failed_total{cluster_type="management_cluster", namespace="mimir"}[5m]) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirIngesterNeedsToBeScaledUp
      annotations:
        description: 'Mimir ingester is consuming too much resources and needs to be scaled up.'
        opsrecipe: mimir-ingester/
      expr: |-
        sum by (cluster_id, installation, namespace, pipeline, provider) (container_memory_working_set_bytes{container="ingester", namespace="mimir"}) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte"}) 
          >= 0.90 
        or 
        sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir"}[5m])) by (cluster_id, installation, namespace, pipeline, provider) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core"}) 
          >= 0.90
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirIngesterNeedsToBeScaledDown
      annotations:
        description: 'Mimir ingester is consuming very few resources and needs to be scaled down.'
        opsrecipe: mimir-ingester/
      expr: |-
        sum by (cluster_id, installation, namespace, pipeline, provider) (container_memory_working_set_bytes{container="ingester", namespace="mimir"}) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte"}) 
          <= 0.30 
        or 
        sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir"}[5m])) by (cluster_id, installation, namespace, pipeline, provider) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core"}) 
          <= 0.30
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
{{- end }}
