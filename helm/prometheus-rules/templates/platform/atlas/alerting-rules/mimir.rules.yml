apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: mimir.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: mimir
    rules:
    # This alert is meant to always fire, to ensure the entire alerting pipeline is functional.
    - alert: "Heartbeat"
      annotations:
        description: This alert is used to ensure the entire alerting pipeline is functional.
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      expr: up{job=~"mimir/.*", container!="prometheus"} > 0
      labels:
        area: platform
        installation: {{ .Values.managementCluster.name }}
        team: atlas
        topic: observability
    # Coming from https://github.com/giantswarm/giantswarm/issues/30124
    # This alert ensures Mimir containers are not restarting too often (flappiness).
    # If it is not the the case, this can incur high costs by cloud providers (s3 api calls are quite expensive).
    # This alert will not page for the prometheus-buddy.
    - alert: MimirRestartingTooOften
      annotations:
        __dashboardUid__: ffcd83628d7d4b5a03d1cafd159e6c9c
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir containers are restarting too often.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      expr: |
        increase(
          kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container!="prometheus"}[1h]
        ) > 5
      for: 5m
      labels:
        area: platform
        # This label is used to ensure the alert go through even for non-stable installations
        all_pipelines: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirComponentDown
      annotations:
        __dashboardUid__: ffcd83628d7d4b5a03d1cafd159e6c9c
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir component : {{ $labels.service }} is down.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      expr: count(up{job=~"mimir/.*", container!="prometheus"} == 0) by (cluster_id, installation, provider, pipeline, service) > 0
      for: 5m
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirRulerEventsFailed
      annotations:
        __dashboardUid__: 631e15d5d85afb2ca8e35d62984eeaa0
        dashboardQueryParams: "orgId=2"
        description: 'Mimir ruler is failing to process PrometheusRules.'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      expr: rate(mimir_rules_events_failed_total{cluster_type="management_cluster", namespace="mimir"}[5m]) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirIngesterNeedsToBeScaledUp
      annotations:
        description: 'Mimir ingester is consuming too much resources and needs to be scaled up.'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-ingester/
      expr: |-
        sum by (cluster_id, installation, namespace, pipeline, provider) (container_memory_working_set_bytes{container="ingester", namespace="mimir"}) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte"}) 
          >= 0.90 
        or 
        sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir"}[5m])) by (cluster_id, installation, namespace, pipeline, provider) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core"}) 
          >= 0.95
      for: 12h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirIngesterNeedsToBeScaledDown
      annotations:
        description: 'Mimir ingester is consuming very few resources and needs to be scaled down.'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-ingester/
      expr: |-
        sum by (cluster_id, installation, namespace, pipeline, provider) (container_memory_working_set_bytes{container="ingester", namespace="mimir"}) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte"}) 
          <= 0.30 
        and
        sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir"}[5m])) by (cluster_id, installation, namespace, pipeline, provider) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core"}) 
          <= 0.30
      for: 30m
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirHPAReachedMaxReplicas
      annotations:
        description: '{{`Mimir {{ $labels.horizontalpodautoscaler }} HPA has reached maximum replicas and consume too much resources, it needs to be scaled up.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-hpa/
      expr: |-
        (
          kube_horizontalpodautoscaler_status_desired_replicas{namespace="mimir"} >=
          on(cluster_id, customer, installation, namespace, horizontalpodautoscaler)
          kube_horizontalpodautoscaler_spec_max_replicas{namespace="mimir"}
        )
        and on(cluster_id, customer, installation, namespace, horizontalpodautoscaler)
        (
          kube_horizontalpodautoscaler_status_target_metric{namespace="mimir"} >
          on(cluster_id, customer, installation, namespace, horizontalpodautoscaler, metric_name, metric_target_type)
          kube_horizontalpodautoscaler_spec_target_metric{namespace="mimir"}
        )
      for: 30m
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirRulerTooManyFailedQueries
      annotations:
        __dashboardUid__: 631e15d5d85afb2ca8e35d62984eeaa0
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir Ruler {{ $labels.pod }} is experiencing {{ printf "%.2f" $value }}% errors while evaluating rules.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      expr: |
        100 * (
        sum by (installation, cluster_id, pipeline, provider, namespace, pod) (rate(cortex_ruler_queries_failed_total[1m]))
          /
        sum by (installation, cluster_id, pipeline, provider, namespace, pod) (rate(cortex_ruler_queries_total[1m]))
        ) > 1
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirDataPushFailures
      annotations:
        __dashboardUid__: e1324ee2a434f4158c00a9ee279d3292
        dashboardQueryParams: "orgId=2"
        description: '{{`Low rate of writes to the Mimir object store.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/#mimirdatapushfailures
      expr: |
        sum by (installation, cluster_id, cluster_type, pipeline, provider) (
          rate(thanos_objstore_bucket_operation_failures_total{cluster_type="management_cluster", namespace="mimir", component="ingester", operation="upload"}[30m])
        )
        /
        sum by (installation, cluster_id, cluster_type, pipeline, provider) (
          rate(thanos_objstore_bucket_operations_total{cluster_type="management_cluster", namespace="mimir", component="ingester", operation="upload"}[30m])
        )
        > 0
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  - name: mimir.compactor
    rules:
    - alert: MimirCompactorFailedCompaction
      annotations:
        __dashboardUid__: 09a5c49e9cdb2f2b24c6d184574a07fd
        dashboardQueryParams: "orgId=2"
        description: 'Mimir compactor has been failing its compactions for 2 hours.'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir#mimircompactorfailedcompaction
      expr: min by (cluster_id, installation, namespace, provider, pipeline) (time() - (cortex_compactor_last_successful_run_timestamp_seconds > 0 ) ) > 60 * 60 * 2
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirCompactorFailedCompaction
      annotations:
        __dashboardUid__: 09a5c49e9cdb2f2b24c6d184574a07fd
        dashboardQueryParams: "orgId=2"
        description: 'Mimir compactor has been failing compactions for more than 2 hours since start-up.'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir#mimircompactorfailedcompaction
      # This alert covers the special case at compactor startup, where the "normal" alert would always consider time `0` is more than 2 hours ago, yet we want to let it 2 hours + `for` duration.
      expr: max(max_over_time(cortex_compactor_last_successful_run_timestamp_seconds{}[2h])) by (cluster_id, installation, namespace, provider, pipeline) == 0
      for: 2h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  - name: mimir.continuous-test
    rules:
    - alert: MimirContinuousTestFailingOnWrites
      annotations:
        __dashboardUid__: mimir-continuous-test
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir continuous test {{ $labels.test }} in {{ $labels.cluster_id }}/{{ $labels.namespace }} is not effectively running because writes are failing.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      # Query is based on the following upstream mixin alerting rule: https://github.com/grafana/mimir/blob/b873372adbf0996bff70de55934f3dd4a10c7b89/operations/mimir-mixin-compiled/alerts.yaml#L1196
      expr: sum by(cluster_id, installation, namespace, pipeline, provider, test) (rate(mimir_continuous_test_writes_failed_total[5m])) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirContinuousTestFailingOnReads
      annotations:
        __dashboardUid__: mimir-continuous-test
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir continuous test {{ $labels.test }} in {{ $labels.cluster_id }}/{{ $labels.namespace }} is not effectively running because queries are failing.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      # Query is based on the following upstream mixin alerting rule: https://github.com/grafana/mimir/blob/b873372adbf0996bff70de55934f3dd4a10c7b89/operations/mimir-mixin-compiled/alerts.yaml#L1185
      expr: sum by(cluster_id, installation, namespace, pipeline, provider, test) (rate(mimir_continuous_test_queries_failed_total[5m])) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirContinuousTestFailing
      annotations:
        __dashboardUid__: mimir-continuous-test
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir continuous test {{ $labels.test }} in {{ $labels.cluster_id }}/{{ $labels.namespace }} is not effectively running because queries are failing.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      # Query is based on the following upstream mixin alerting rule: https://github.com/grafana/mimir/blob/b873372adbf0996bff70de55934f3dd4a10c7b89/operations/mimir-mixin-compiled/alerts.yaml#L1205
      expr: sum by(cluster_id, installation, pipeline, provider, namespace, test) (rate(mimir_continuous_test_query_result_checks_failed_total[10m])) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirContinuousTestMissing
      annotations:
        __dashboardUid__: mimir-continuous-test
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir continuous test {{ $labels.cluster_id }} is not producing metrics.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      expr: |
        sum by(cluster_id, installation, pipeline, provider) (
          rate(mimir_continuous_test_writes_total[10m]) == 0
          or absent(
            mimir_continuous_test_writes_total{
              cluster_type="management_cluster",
              cluster_id="{{ .Values.managementCluster.name }}",
              installation="{{ .Values.managementCluster.name }}",
              provider="{{ .Values.managementCluster.provider.kind }}",
              pipeline="{{ .Values.managementCluster.pipeline }}"
            }
          )
        )
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        inhibit_metrics_broken: "true"
        severity: page
        team: atlas
        topic: observability
  # The following alerts are taken from upstream Mimir https://github.com/grafana/mimir/blob/59868b5e4ad5562a836e057a21c37b7e8067ae7d/operations/mimir-mixin-compiled/alerts.yaml#L617
  # Feel free to adapt as you see fit.
  - name: mimir-alertmanager
    rules:
    - alert: MimirAlertmanagerSyncConfigsFailing
      annotations:
        __dashboardUid__: b0d38d318bbddd80476246d4930f9e55
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir Alertmanager {{ $labels.pod }} is failing to read tenant configurations from storage.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimiralertmanagersyncconfigsfailing
      expr: |
        rate(cortex_alertmanager_sync_configs_failed_total[5m]) > 0
      for: 30m
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirAlertmanagerRingCheckFailing
      annotations:
        __dashboardUid__: b0d38d318bbddd80476246d4930f9e55
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir Alertmanager {{ $labels.pod }} is unable to check tenants ownership via the ring.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimiralertmanagerringcheckfailing
      expr: |
        rate(cortex_alertmanager_ring_check_errors_total[2m]) > 0
      for: 10m
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirAlertmanagerReplicationFailing
      annotations:
        __dashboardUid__: b0d38d318bbddd80476246d4930f9e55
        dashboardQueryParams: "orgId=2"
        description: '{{`Mimir Alertmanager {{ $labels.pod }} is failing to replicating partial state to its replicas.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimiralertmanagerreplicationfailing
      expr: |
        rate(cortex_alertmanager_state_replication_failed_total[2m]) > 0
      for: 10m
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
