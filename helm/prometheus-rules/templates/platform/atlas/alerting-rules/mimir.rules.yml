{{- if .Values.mimir.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: mimir.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: mimir
    rules:
    # This alert is meant to always fire, to ensure the entire alerting pipeline is functional.
    - alert: "Heartbeat"
      annotations:
        description: This alert is used to ensure the entire alerting pipeline is functional.
        opsrecipe: mimir/
      expr: up{job=~"mimir/.*", container!="prometheus"} > 0
      labels:
        area: platform
        installation: {{ .Values.managementCluster.name }}
        # TODO(@giantswarm/team-atlas): We need this label as long as we have the old and new heartbeats. Let's remove once the legacy monitoring is gone
        type: mimir-heartbeat
        team: atlas
        topic: observability
    # Coming from https://github.com/giantswarm/giantswarm/issues/30124
    # This alert ensures Mimir containers are not restarting too often (flappiness).
    # If it is not the the case, this can incur high costs by cloud providers (s3 api calls are quite expensive).
    # This alert will not page for the prometheus-buddy.
    - alert: MimirRestartingTooOften
      annotations:
        dashboard: ffcd83628d7d4b5a03d1cafd159e6c9c/mimir-overview
        description: '{{`Mimir containers are restarting too often.`}}'
        opsrecipe: mimir/
      expr: |
        increase(
          kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container!="prometheus"}[1h]
        ) > 5
      for: 5m
      labels:
        area: platform
        # This label is used to ensure the alert go through even for non-stable installations
        all_pipelines: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirComponentDown
      annotations:
        dashboard: ffcd83628d7d4b5a03d1cafd159e6c9c/mimir-overview
        description: '{{`Mimir component : {{ $labels.service }} is down.`}}'
        opsrecipe: mimir/
      expr: count(up{job=~"mimir/.*", container!="prometheus"} == 0) by (cluster_id, installation, provider, pipeline, service) > 0
      for: 5m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: AlloyForPrometheusRulesDown
      annotations:
        description: 'Alloy sending PrometheusRules to Mimir ruler is down.'
        opsrecipe: prometheus-rules/
      expr: count(up{job="alloy-rules", namespace="mimir"} == 0) by (cluster_id, installation, provider, pipeline) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirRulerEventsFailed
      annotations:
        dashboard: 631e15d5d85afb2ca8e35d62984eeaa0/mimir-ruler
        description: 'Mimir ruler is failing to process PrometheusRules.'
        opsrecipe: mimir/
      expr: rate(mimir_rules_events_failed_total{cluster_type="management_cluster", namespace="mimir"}[5m]) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirIngesterNeedsToBeScaledUp
      annotations:
        description: 'Mimir ingester is consuming too much resources and needs to be scaled up.'
        opsrecipe: mimir-ingester/
      expr: |-
        sum by (cluster_id, installation, namespace, pipeline, provider) (container_memory_working_set_bytes{container="ingester", namespace="mimir"}) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte"}) 
          >= 0.90 
        or 
        sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir"}[5m])) by (cluster_id, installation, namespace, pipeline, provider) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core"}) 
          >= 0.90
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirIngesterNeedsToBeScaledDown
      annotations:
        description: 'Mimir ingester is consuming very few resources and needs to be scaled down.'
        opsrecipe: mimir-ingester/
      expr: |-
        sum by (cluster_id, installation, namespace, pipeline, provider) (container_memory_working_set_bytes{container="ingester", namespace="mimir"}) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte"}) 
          <= 0.30 
        and
        sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir"}[5m])) by (cluster_id, installation, namespace, pipeline, provider) 
          / 
        sum by(cluster_id, installation, namespace, pipeline, provider) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core"}) 
          <= 0.30
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirHPAReachedMaxReplicas
      annotations:
        description: '{{`Mimir {{ $labels.horizontalpodautoscaler }} HPA has reached maximum replicas and consume too much resources, it needs to be scaled up.`}}'
        opsrecipe: mimir-hpa/
      expr: |-
        (
          kube_horizontalpodautoscaler_status_desired_replicas{namespace="mimir"} >=
          on(cluster_id, customer, installation, namespace, horizontalpodautoscaler)
          kube_horizontalpodautoscaler_spec_max_replicas{namespace="mimir"}
        )
        and on(cluster_id, customer, installation, namespace, horizontalpodautoscaler)
        (
          kube_horizontalpodautoscaler_status_target_metric{namespace="mimir"} >
          on(cluster_id, customer, installation, namespace, horizontalpodautoscaler, metric_name, metric_target_type)
          kube_horizontalpodautoscaler_spec_target_metric{namespace="mimir"}
        )
      for: 30m
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
  - name: mimir.compactor
    rules:
    - alert: MimirCompactorFailedCompaction
      annotations:
        dashboard: 09a5c49e9cdb2f2b24c6d184574a07fd/mimir-compactor-resources
        description: 'Mimir compactor has been failing its compactions for 2 hours.'
        opsrecipe: mimir#mimircompactorfailedcompaction
      # Query is based on the following upstream mixin alerting rule : https://github.com/grafana/mimir/blob/main/operations/mimir-mixin-compiled/alerts.yaml#L858
      expr: sum(increase(cortex_compactor_runs_failed_total{reason!="shutdown"}[2h])) by (cluster_id, installation, namespace, pipeline, provider) > 2
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: MimirContinuousTestFailingOnWrites
      annotations:
        description: 'Mimir continous-test detected errors in the write path.'
        opsrecipe: mimir/
      # Query is based on the following upstream mixin alerting rule : https://github.com/grafana/mimir/blob/main/operations/mimir-mixin-compiled/alerts.yaml#L1097
      expr: sum by(cluster_id, installation, namespace, pipeline, provider, test) (rate(mimir_continuous_test_writes_failed_total[5m])) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        #TODO dashboard:
        severity: page
        team: atlas
        topic: observability
    - alert: MimirContinuousTestFailingOnReads
      annotations:
        description: 'Mimir continous-test detected errors in the write path.'
        opsrecipe: mimir/
      # Query is based on the following upstream mixin alerting rule : https://github.com/grafana/mimir/blob/main/operations/mimir-mixin-compiled/alerts.yaml#L1097
      expr: sum by(cluster_id, installation, namespace, pipeline, provider, test) (rate(mimir_continuous_test_queries_failed_total[5m])) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"
        cancel_if_cluster_status_updating: "true"
        cancel_if_outside_working_hours: "true"
        # TODO dashboard: 
        severity: page
        team: atlas
        topic: observability
{{- end }}
