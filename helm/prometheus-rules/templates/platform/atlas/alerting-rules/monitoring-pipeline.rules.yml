apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: monitoring-pipeline.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: monitoring-pipeline
    rules:
    - alert: MetricForwardingErrors
      annotations:
        description: '{{`Monitoring agent can''t communicate with Remote Storage API at {{ $labels.url }}.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-pipeline/
        __dashboardUid__: promRW001
        {{ if eq .Values.managementCluster.provider.flavor "capi" }}
        dashboardQueryParams: "orgId=1"
        {{ end }}
      expr: |-
        rate(prometheus_remote_storage_samples_failed_total[10m]) > 0.1
          or rate(prometheus_remote_storage_samples_total[10m]) == 0
          or rate(prometheus_remote_storage_metadata_retried_total[10m]) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: JobScrapingFailure
      annotations:
        __dashboardUid__: servicemonitors-details
        {{ if eq .Values.managementCluster.provider.flavor "capi" }}
        dashboardQueryParams: "orgId=1"
        {{ end }}
        description: '{{`Monitoring agents for cluster {{$labels.installation}}/{{$labels.cluster_id}} has failed to scrape all targets in {{$labels.job}} job.`}}'
        summary: Monitoring agent failed to scrape all targets in a job.
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
      expr: |-
        (
          count(up == 0) by (job, installation, cluster_id, provider, pipeline)
          /
          count(up) by (job, installation, cluster_id, provider, pipeline)
        ) >= 1
      for: 1d
      labels:
        area: platform
        severity: notify
        team: atlas
        topic: observability
        cancel_if_outside_working_hours: "true"
    - alert: CriticalJobScrapingFailure
      annotations:
        __dashboardUid__: servicemonitors-details
        {{ if eq .Values.managementCluster.provider.flavor "capi" }}
        dashboardQueryParams: "orgId=1"
        {{ end }}
        description: '{{`Monitoring agents for cluster {{$labels.installation}}/{{$labels.cluster_id}} has failed to scrape all targets in {{$labels.job}} job.`}}'
        summary: Monitoring agent failed to scrape all targets in a job.
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
      ## We ignore bastion hosts node exporters
      expr: |-
        (
          count(
            (
              up{job=~".*(apiserver|kube-controller-manager|kube-scheduler|node-exporter|kube-state-metrics).*"}
              or
              up{job="kubelet", metrics_path="/metrics"}
            ) == 0
          ) by (job, installation, cluster_id, provider, pipeline)
          /
          count(
            up{job=~".*(apiserver|kube-controller-manager|kube-scheduler|node-exporter|kube-state-metrics).*"}
            or
            up{job="kubelet", metrics_path="/metrics"}
          ) by (job, installation, cluster_id, provider, pipeline)
        ) >= 1
      for: 3d
      labels:
        area: platform
        severity: page
        team: atlas
        topic: observability
        cancel_if_outside_working_hours: "true"
        cancel_if_cluster_is_not_running_monitoring_agent: "true"
        cancel_if_cluster_status_creating: "true"
        cancel_if_cluster_status_deleting: "true"

