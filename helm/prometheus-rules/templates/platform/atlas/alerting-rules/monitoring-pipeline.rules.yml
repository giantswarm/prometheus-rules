apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: monitoring-pipeline.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: monitoring-pipeline
    rules:
    - alert: MetricForwardingErrors
      annotations:
        summary: Monitoring agent fails to send samples to remote storage.
        description: '{{`Monitoring agent failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-pipeline/
        __dashboardUid__: promRW001
        dashboardQueryParams: "orgId=1"
      expr: |-
        (
          rate(prometheus_remote_storage_samples_failed_total[5m])
          /
          (
            rate(prometheus_remote_storage_samples_failed_total[5m])
            +
            rate(prometheus_remote_storage_samples_total[5m])
          )
        ) * 100
        > 10
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: JobScrapingFailure
      annotations:
        __dashboardUid__: servicemonitors-details
        dashboardQueryParams: "orgId=1"
        description: '{{`Monitoring agents for cluster {{$labels.installation}}/{{$labels.cluster_id}} has failed to scrape all targets in {{$labels.job}} job.`}}'
        summary: Monitoring agent failed to scrape all targets in a job.
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
      expr: |-
        (
          count(up == 0) by (job, installation, cluster_id, provider, pipeline)
          /
          count(up) by (job, installation, cluster_id, provider, pipeline)
        ) >= 1
      for: 1d
      labels:
        area: platform
        severity: notify
        team: atlas
        topic: observability
        cancel_if_outside_working_hours: "true"
    - alert: CriticalJobScrapingFailure
      annotations:
        __dashboardUid__: servicemonitors-details
        dashboardQueryParams: "orgId=1"
        description: '{{`Monitoring agents for cluster {{$labels.installation}}/{{$labels.cluster_id}} has failed to scrape all targets in {{$labels.job}} job.`}}'
        summary: Monitoring agent failed to scrape all targets in a job.
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
      ## We ignore bastion hosts node exporters
      expr: |-
        (
          count(
            (
              up{job=~".*(apiserver|kube-controller-manager|kube-scheduler|node-exporter|kube-state-metrics).*"}
              or
              up{job="kubelet", metrics_path="/metrics"}
            ) == 0
          ) by (job, installation, cluster_id, provider, pipeline)
          /
          count(
            up{job=~".*(apiserver|kube-controller-manager|kube-scheduler|node-exporter|kube-state-metrics).*"}
            or
            up{job="kubelet", metrics_path="/metrics"}
          ) by (job, installation, cluster_id, provider, pipeline)
        ) >= 1
      for: 3d
      labels:
        area: platform
        severity: page
        team: atlas
        topic: observability
        cancel_if_outside_working_hours: "true"
