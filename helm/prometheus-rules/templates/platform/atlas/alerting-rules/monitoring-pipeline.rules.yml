apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: monitoring-pipeline.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: monitoring-pipeline
    rules:
    - alert: MetricForwardingErrors
      annotations:
        summary: Monitoring agent fails to send samples to remote storage.
        description: '{{`Monitoring agent failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}.`}}'
        runbook_url: '{{`https://intranet.giantswarm.io/docs/support-and-ops/runbooks/monitoring-pipeline/?INSTALLATION={{ $labels.installation }}&CLUSTER={{ $labels.cluster_id }}`}}'
        __dashboardUid__: promRW001
        dashboardQueryParams: "orgId=1"
      expr: |-
        (
          rate(prometheus_remote_storage_samples_failed_total[5m])
          /
          (
            rate(prometheus_remote_storage_samples_failed_total[5m])
            +
            rate(prometheus_remote_storage_samples_total[5m])
          )
        ) * 100
        > 10
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: JobScrapingFailure
      annotations:
        __dashboardUid__: servicemonitors-details
        dashboardQueryParams: "orgId=1"
        description: '{{`Monitoring agents for cluster {{$labels.installation}}/{{$labels.cluster_id}} has failed to scrape all targets in {{$labels.job}} job.`}}'
        summary: Monitoring agent failed to scrape all targets in a job.
        runbook_url: '{{`https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION={{ $labels.installation }}&CLUSTER={{ $labels.cluster_id }}`}}'
      expr: |-
        # This alert uses the same logic as the `aggregation:giantswarm:jobscrapingfailures` recording rule
        (
          (
            count(up == 0) by (app, job, installation, cluster_id, provider, pipeline)
            /
            count(up) by (app, job, installation, cluster_id, provider, pipeline)
          )
          * on(app, cluster_id) group_left(team)
            label_replace(app_operator_app_info, "app", "$1", "name", "[^-]+-(.+)")
        ) >= 1
      for: 1d
      labels:
        area: platform
        severity: notify
        topic: observability
        cancel_if_outside_working_hours: "true"
    - alert: CriticalJobScrapingFailure
      annotations:
        __dashboardUid__: servicemonitors-details
        dashboardQueryParams: "orgId=1"
        description: '{{`Monitoring agents for cluster {{$labels.installation}}/{{$labels.cluster_id}} has failed to scrape all targets in {{$labels.job}} job.`}}'
        summary: Monitoring agent failed to scrape all targets in a job.
        runbook_url: '{{`https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION={{ $labels.installation }}&CLUSTER={{ $labels.cluster_id }}`}}'
      ## We ignore bastion hosts node exporters
      expr: |-
        (
          count(
            (
              up{job=~".*(apiserver|kube-controller-manager|kube-scheduler|node-exporter|kube-state-metrics).*"}
              or
              up{job="kubelet", metrics_path="/metrics"}
            ) == 0
          ) by (job, installation, cluster_id, provider, pipeline)
          /
          count(
            up{job=~".*(apiserver|kube-controller-manager|kube-scheduler|node-exporter|kube-state-metrics).*"}
            or
            up{job="kubelet", metrics_path="/metrics"}
          ) by (job, installation, cluster_id, provider, pipeline)
        ) >= 1
      for: 3d
      labels:
        area: platform
        severity: page
        team: atlas
        topic: observability
        cancel_if_outside_working_hours: "true"
