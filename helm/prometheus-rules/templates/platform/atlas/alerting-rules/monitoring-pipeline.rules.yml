apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: monitoring-pipeline.rules
  namespace: {{ .Values.namespace }}
spec:
  groups:
  - name: monitoring-pipeline
    rules:
    - alert: MetricForwardingErrors
      annotations:
        summary: Monitoring agent fails to send samples to remote storage.
        description: '{{`Monitoring agent failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}.`}}'
        runbook_url: '{{`https://intranet.giantswarm.io/docs/support-and-ops/runbooks/monitoring-pipeline/?INSTALLATION={{ $labels.installation }}&CLUSTER={{ $labels.cluster_id }}`}}'
        __dashboardUid__: promRW001
        dashboardQueryParams: "orgId=1"
      expr: |-
        (
          rate(prometheus_remote_storage_samples_failed_total[5m])
          /
          (
            rate(prometheus_remote_storage_samples_failed_total[5m])
            +
            rate(prometheus_remote_storage_samples_total[5m])
          )
        ) * 100
        > 10
      for: 1h
      labels:
        area: platform
        cancel_if_outside_working_hours: "true"
        severity: page
        team: atlas
        topic: observability
    - alert: JobScrapingFailure
      annotations:
        __dashboardUid__: alloy-metrics-targets
        dashboardQueryParams: "orgId=1"
        summary: Monitoring agent failed to scrape all targets for an app / job.
        description: '{{`No targets are reachable for {{$labels.app}} by monitoring agent on cluster {{$labels.installation}}/{{$labels.cluster_id}} (job: {{$labels.job}})`}}'
        runbook_url: '{{`https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION={{ $labels.installation }}&CLUSTER={{ $labels.cluster_id }}`}}'
      # Alert fires when 100% of an application instances are down (complete service outage).
      # Calculates the ratio of failed scrape targets (up==0) to total targets per app/job.
      # Joins with app_operator_app_info to include team ownership for proper alert routing.
      # Only monitors apps deployed in org namespaces (org-*).
      expr: |-
        # This alert uses the same logic as the `aggregation:giantswarm:jobscrapingfailures` recording rule
        (
          (
            count(label_join(up, "app", "-", "cluster_id", "app") == 0) by (app, job, installation, cluster_id, provider, pipeline)
            /
            count(label_join(up, "app", "-", "cluster_id", "app")) by (app, job, installation, cluster_id, provider, pipeline)
          )
          * on(app, cluster_id) group_left(team)
            label_replace(app_operator_app_info{namespace=~"org-.+"}, "app", "$1", "name", "(.+)")
        ) >= 1
      for: 1h
      labels:
        area: platform
        severity: notify
        topic: observability
        cancel_if_outside_working_hours: "true"
    - alert: JobScrapingFailureMC
      annotations:
        __dashboardUid__: alloy-metrics-targets
        dashboardQueryParams: "orgId=1"
        summary: Monitoring agent failed to scrape all targets for an app / job on a management cluster.
        description: '{{`No targets are reachable for {{$labels.app}} by monitoring on management cluster {{$labels.installation}} (job: {{$labels.job}})`}}'
        runbook_url: '{{`https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION={{ $labels.installation }}&CLUSTER={{ $labels.cluster_id }}`}}'
      # Alert fires when 100% of application instances are down (complete service outage).
      # Calculates the ratio of failed scrape targets (up==0) to total targets per app/job.
      # Joins with app_operator_app_info to include team ownership for proper alert routing.
      # Only monitors apps deployed in non org namespaces (NOT org-*).
      expr: |-
        # This alert uses the same logic as the `aggregation:giantswarm:jobscrapingfailures` recording rule
        (
          (
            count(up == 0) by (app, job, installation, cluster_id, provider, pipeline)
            /
            count(up) by (app, job, installation, cluster_id, provider, pipeline)
          )
          * on(app, cluster_id) group_left(team)
            label_replace(app_operator_app_info{namespace!~"org-.+"}, "app", "$1", "name", "(.+)")
        ) >= 1
      for: 1h
      labels:
        area: platform
        severity: notify
        topic: observability
        cancel_if_outside_working_hours: "true"
