{{- if eq .Values.managementCluster.provider.flavor "vintage" }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: prometheus-meta-operator
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: observability
    rules:
    - alert: "Heartbeat"
      expr: up{job=~".*prometheus/prometheus.*",instance!="prometheus-agent"} == 1
      labels:
        area: platform
        installation: {{ .Values.managementCluster.name }}
        team: atlas
        topic: observability
        type: heartbeat
        namespace: "monitoring" # Needed due to https://github.com/prometheus-operator/prometheus-operator/issues/3737
      annotations:
        description: This alert is used to ensure the entire alerting pipeline is functional.
    - alert: "MatchingNumberOfPrometheusAndCluster"
      annotations:
        description: This alert is used to ensure we have as many workload cluster prometheus as we have workload cluster CR.
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/matching-number-of-prometheus-and-cluster/
      # This expression list all the cluster IDs that exist and are not being deleted and compares them (using unless) to the running prometheus pods.
      # If a prometheus is missing, this alert will fire. This alert will not check if a prometheus is running when it should not (e.g. deleted cluster)
      expr: |
        (
          sum by(cluster_id, installation, provider, pipeline) (
            {__name__=~"cluster_service_cluster_info|cluster_operator_cluster_status", status!="Deleting"}
          ) > 0 unless sum by(cluster_id, installation, provider, pipeline) (
            label_replace(
              kube_pod_container_status_running{container="prometheus", namespace!="{{ .Values.managementCluster.name }}-prometheus", namespace=~".*-prometheus"},
              "cluster_id", "$2", "pod", "(prometheus-)(.+)(-.+)"
            )
          )
        ) or (
          sum by (cluster_id, installation, provider, pipeline) (
            label_replace(capi_cluster_status_phase{phase!="Deleting"},
            "cluster_id", "$1", "name", "(.+)"
            )
          ) unless sum by (cluster_id, installation, provider, pipeline) (
            label_replace(kube_pod_container_status_running{container="prometheus",namespace=~".*-prometheus"},
            "cluster_id", "$2", "pod", "(prometheus-)(.+)(-.+)"
            )
          )
        )
        > 0
      for: 10m
      labels:
        area: platform
        cancel_if_mc_kube_state_metrics_down: "true"
        cancel_if_outside_working_hours: {{ include "workingHoursOnly" . }}
        installation: {{ .Values.managementCluster.name }}
        severity: page
        team: atlas
        topic: observability
    - alert: "PrometheusMetaOperatorReconcileErrors"
      annotations:
        description: '{{`prometheus-meta-operator controller {{ $labels.controller }} too many reconcile errors.`}}'
        runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/pmo-reconcile-errors/
        __dashboardUid__: piJK9Vm4z
      expr: |
        avg_over_time(operatorkit_controller_errors_total{job="monitoring/prometheus-meta-operator"}[20m]) > 0
      for: 1h
      labels:
        area: platform
        cancel_if_mc_kube_state_metrics_down: "false"
        cancel_if_outside_working_hours: "true"
        installation: {{ .Values.managementCluster.name }}
        severity: page
        team: atlas
        topic: observability
{{- end }}
