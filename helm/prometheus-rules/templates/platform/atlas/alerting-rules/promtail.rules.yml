apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: promtail.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
    - name: promtail
      rules:
        # This alert lists the existing promtail pods (to extract the node label and inhibit if the node is not ready)
        # and join the pods with the not running containers
        - alert: PromtailDown
          annotations:
            description: '{{`Scraping of all promtail pods to check if one failed every 30 minutes.`}}'
            runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/promtail/
          expr: |-
            kube_pod_info{pod=~"promtail.*", namespace="kube-system"}
            * on(cluster_id, pod)
              group_left ()
              up{container="promtail", namespace="kube-system"} == 0
          for: 30m
          labels:
            area: platform
            severity: page
            team: atlas
            topic: observability
            cancel_if_outside_working_hours: "true"
            cancel_if_cluster_status_creating: "true"
            cancel_if_cluster_status_deleting: "true"
            cancel_if_cluster_status_updating: "true"
            cancel_if_node_unschedulable: "true"
            cancel_if_node_not_ready: "true"
        - alert: PromtailRequestsErrors
          annotations:
            __dashboardUid__: promtail-overview
            {{ if eq .Values.managementCluster.provider.flavor "capi" }}
            dashboardQueryParams: "orgId=2"
            {{ end }}
            description: This alert checks if that the amount of failed requests is below 10% for promtail
            runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/promtail/
          expr: |
            100 * (
                sum(rate(promtail_request_duration_seconds_count{status_code!~"2.."}[5m])) by (cluster_id, installation, provider, pipeline, namespace, job, route, instance)
                /
                sum(rate(promtail_request_duration_seconds_count[5m])) by (cluster_id, installation, provider, pipeline, namespace, job, route, instance)
            ) > 10
            # ignore this alert when alloy-logs is installed - this avoids false alerts when an unconfigured and unused promtail remains after migration to alloy-logs
            unless on (cluster_id) count(up{service="alloy-logs"}) by (cluster_id) > 0
          for: 60m
          labels:
            area: platform
            severity: page
            team: atlas
            topic: observability
            cancel_if_outside_working_hours: "true"
            cancel_if_cluster_status_creating: "true"
            cancel_if_cluster_status_deleting: "true"
            cancel_if_cluster_status_updating: "true"
        - alert: PromtailConflictsWithAlloy
          # This alert was created as a workaround of https://github.com/giantswarm/giantswarm/issues/33341
          # We are replacing promtail with alloy-logs. This is done during cluster upgrades.
          # Sometimes the old promtail is not uninstalled properly, so we need this alert to inform us.
          annotations:
            __dashboardUid__: promtail-overview
            {{ if eq .Values.managementCluster.provider.flavor "capi" }}
            dashboardQueryParams: "orgId=2"
            {{ end }}
            description: Both promtail and alloy-logs are installed and conflicting on the cluster
            runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/promtail/#check-that-it-does-not-conflict-with-alloy-logs
          expr: |
            count(up{service="alloy-logs"}) by (cluster_id, installation, provider, pipeline) > 0
            and count(up{service="promtail-metrics"}) by (cluster_id, installation, provider, pipeline) > 0
          for: 60m
          labels:
            area: platform
            severity: page
            team: atlas
            topic: observability
            cancel_if_outside_working_hours: "true"
            cancel_if_cluster_status_creating: "true"
            cancel_if_cluster_status_deleting: "true"
            cancel_if_cluster_status_updating: "true"
