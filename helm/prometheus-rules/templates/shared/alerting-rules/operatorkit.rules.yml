apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  creationTimestamp: null
  labels:
    {{- include "labels.common" . | nindent 4 }}
  name: operatorkit.rules
  namespace: {{ .Values.namespace  }}
spec:
  groups:
  - name: operatorkit
    rules:
    - alert: OperatorkitErrorRateTooHighHoneybadger
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }} has reported errors. Please check logs.`}}'
        opsrecipe: check-operator-error-rate-high/
      expr: operatorkit_controller_error_total{pod=~"app-operator.*|chart-operator.*"} > 5
      for: 1m
      labels:
        area: kaas
        severity: notify
        team: honeybadger
        topic: qa
    - alert: OperatorNotReconcilingHoneybadger
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }} not reconciling controller {{$labels.controller}}. Please check logs.`}}'
      expr: (time() - operatorkit_controller_last_reconciled{pod=~"app-operator.*|chart-operator.*"}) / 60 > 30
      for: 10m
      labels:
        area: managedservices
        severity: notify
        team: honeybadger
        topic: releng
    - alert: OperatorkitErrorRateTooHighPhoenix
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }}@{{ $labels.app_version }} has reported errors. Please check the logs.`}}'
        opsrecipe: check-operator-error-rate-high/
      expr: rate(operatorkit_controller_error_total{pod=~"aws-.*"}[5m]) > 1
      for: 10m
      labels:
        area: kaas
        severity: page
        team: phoenix
        cancel_if_outside_working_hours: "true"
        topic: qa
    # In case something stops an operator from reconciling CRs we want to
    # be paged to be able to fix the issue immediately.
    - alert: OperatorkitErrorRateTooHighAWS
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }}@{{ $labels.app_version }} has reported errors. Please check the logs.`}}'
        opsrecipe: check-operator-error-rate-high/
      expr: operatorkit_controller_error_total{pod=~"aws-operator.+|cluster-operator.+"} > 5
      for: 1m
      labels:
        area: kaas
        severity: notify
        team: phoenix
        topic: qa
    # Phoenix
    # It might happen that CRs get orphaned or deletion gets kind of stuck during
    # the cleanup process. Then we want to get notified and figure out what went
    # wrong to fix the root cause eventually.
    - alert: OperatorkitCRNotDeletedAWS
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }}@{{ $labels.app_version }} has not deleted object {{ $labels.namespace }}/{{ $labels.name }} of type {{ $labels.kind }} for too long.`}}'
        opsrecipe: check-not-deleted-object/
      expr: (time() - operatorkit_controller_deletion_timestamp{pod=~"aws-operator.+|cluster-operator.+", provider="aws"}) > 18000
      for: 5m
      labels:
        area: kaas
        severity: notify
        team: phoenix
        topic: qa
    # In case something stops an operator from reconciling CRs we want to
    # be paged to be able to fix the issue immediately.
    - alert: OperatorNotReconcilingAWS
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }}@{{ $labels.app_version }} has stopped the reconciliation. Please check logs.`}}'
        opsrecipe: operator-not-reconciling/
      expr: (sum by (cluster_id, installation, pipeline, provider, instance, pod, app_version, namespace)(increase(operatorkit_controller_event_count{pod=~"aws-operator.+|cluster-operator.+"}[10m])) == 0 and on (cluster_id, instance) (operatorkit_controller_deletion_timestamp or operatorkit_controller_creation_timestamp))
      for: 20m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        severity: notify
        team: phoenix
        topic: qa

    # In case something stops an operator from reconciling CRs we want to
    # be paged to be able to fix the issue immediately.
    - alert: OperatorkitErrorRateTooHighKaas
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }}@{{ $labels.app_version }} has reported errors. Please check the logs.`}}'
        opsrecipe: check-operator-error-rate-high/
      expr: operatorkit_controller_error_total{pod=~"ignition-operator.*|cert-operator.*|node-operator.*"} > 5
      for: 1m
      labels:
        area: kaas
        severity: notify
        team: {{ include "providerTeam" . }}
        topic: qa
    # In case something stops an operator from reconciling CRs we want to
    # be paged to be able to fix the issue immediately.
    - alert: OperatorNotReconcilingProviderTeam
      annotations:
        description: '{{`{{ $labels.namespace }}/{{ $labels.pod }}@{{ $labels.app_version }} has stopped the reconciliation. Please check logs.`}}'
        opsrecipe: operator-not-reconciling/
      expr: (sum by (cluster_id, installation, pipeline, provider, instance, pod, app_version, namespace)(increase(operatorkit_controller_event_count{pod=~"node-operator.*"}[10m])) == 0 and on (cluster_id, instance) (operatorkit_controller_deletion_timestamp or operatorkit_controller_creation_timestamp))
      for: 20m
      labels:
        area: kaas
        cancel_if_outside_working_hours: "true"
        severity: page
        team: {{ include "providerTeam" . }}
        topic: qa
