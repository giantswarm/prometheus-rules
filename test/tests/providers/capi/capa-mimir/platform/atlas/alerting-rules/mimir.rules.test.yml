---
rule_files:
  - mimir.rules.yml

tests:
  - interval: 1m
    input_series:
      # For the first 60min: test with 1 pod: up, none, up, down, up
      - series: 'up{job="mimir/ingester", container="ingester"}'
        values: "1+0x60 _x30 1+0x30 0+0x30 1+0x30"
    alert_rule_test:
      - alertname:  Heartbeat
        eval_time: 20m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              opsrecipe: "mimir/"
      - alertname:  Heartbeat
        eval_time: 70m
      - alertname:  Heartbeat
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              opsrecipe: "mimir/"
      - alertname:  Heartbeat
        eval_time: 140m
      - alertname:  Heartbeat
        eval_time: 165m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              opsrecipe: "mimir/"
  - interval: 1m
    input_series:
      # For the first 60min: test with 1 pod: none, up, down
      - series: 'up{job="mimir/ingester", container="ingester", cluster_type="management_cluster", cluster_id="gauss", installation="gauss", provider="aws", pipeline="testing", service="mimir-ingester"}'
        values: "_x20 1+0x20 0+0x20"
    alert_rule_test:
      - alertname:  MimirComponentDown
        eval_time: 10m
      - alertname:  MimirComponentDown
        eval_time: 30m
      - alertname:  MimirComponentDown
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              service: mimir-ingester
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: gauss
              installation: gauss
              provider: aws
              pipeline: testing
            exp_annotations:
              description: "Mimir component : mimir-ingester is down."
              opsrecipe: "mimir/"
  - interval: 1m
    input_series:
      # test with 1 pod: none, up, down
      - series: 'up{job="grafana-agent-rules", cluster_type="management_cluster", cluster_id="golem", provider="capa", pipeline="testing", installation="golem", namespace="mimir"}'
        values: "_x20 1+0x70 0+0x70"
    alert_rule_test:
      - alertname: GrafanaAgentForPrometheusRulesDown
        eval_time: 10m
      - alertname: GrafanaAgentForPrometheusRulesDown
        eval_time: 80m
      - alertname: GrafanaAgentForPrometheusRulesDown
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cluster_id: golem
              installation: golem
              provider: capa
              pipeline: testing
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: "Grafana-agent sending PrometheusRules to Mimir ruler is down."
              opsrecipe: "mimir/"
  - interval: 1m
    input_series:
      # test: none, rate > 0, rate = 0
      - series: 'mimir_rules_events_failed_total{cluster_type="management_cluster", cluster_id="golem", installation="golem", namespace="mimir"}'
        values: "_x20 1+1x80 0+0x70"
    alert_rule_test:
      - alertname: MimirRulerEventsFailed
        eval_time: 40m
      - alertname: MimirRulerEventsFailed
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cluster_id: golem
              cluster_type: management_cluster
              installation: golem
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: "Mimir ruler is failing to process PrometheusRules."
              opsrecipe: "mimir/"
      - alertname: MimirRulerEventsFailed
        eval_time: 160m
  - interval: 1m
    input_series:
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container="mimir-ingester"}'
        values: "0+0x20 0+5x20 100+0x140" # 0 restarts after 20 minutes then we restart 5 times per minute for 20 minutes then we stop restarting for 140 minutes
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container="prometheus"}'
        values: "0+5x180"                 # prometheus container restarts 5 times per minute for 180 minutes
    alert_rule_test:
      - alertname: MimirRestartingTooOften
        eval_time: 15m  # should be OK after 15 minutes
      - alertname: MimirRestartingTooOften
        eval_time: 85m  # After 85 minutes, should fire an alert for the t+85 error
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir containers are restarting too often.
              opsrecipe: "mimir/"
      - alertname: MimirRestartingTooOften
        eval_time: 140m  # After 140m minutes, all should be back to normal
  # Test for MimirIngesterNeedsToBeScaledUp alert
  - interval: 1m
    input_series:
      - series: 'sum by (namespace) (container_memory_working_set_bytes{container="ingester", namespace="mimir", cluster_type="management_cluster"}'
        values: "15+0x20 23+0x40 16+0x140 23+0x40 15+0x60" # mimir-ingester real memory usage gradually increases until it goes beyond 90% of the memory requests.
      - series: 'sum by(namespace) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster"}'
        values: "24+0x300"                                 # mimir-ingester memory requests stay the same for the entire duration of the test.
      - series: 'sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir", cluster_type="management_cluster"}[5m])) by (namespace)'
        values: "1+0x100 2.8+0x40 1+0x60 2.8+0x40 1+0x60"  # mimir-ingester real cpu usage gradually increases until it goes beyond 90% of the cpu requests.
      - series: 'sum by(namespace) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster"})'
        values: "3+0x300"                                  # mimir-ingester cpu requests stay the same for the entire duration of the test
    alert_rule_test:
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 15m  # should be OK after 15 minutes
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 55m  # After 55 minutes, should fire an alert
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir/"
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 100m  # After 140m minutes, all should be back to normal
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 135m  # After 55 minutes, should fire an alert
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir/"
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 180m  # After 140m minutes, all should be back to normal
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 235m  # After 55 minutes, should fire an alert
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir/"
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 280m  # After 140m minutes, all should be back to normal
  # Test for MimirIngesterNeedsToBeScaledDown alert
  - interval: 1m
    input_series:
      - series: 'sum by (namespace) (container_memory_working_set_bytes{container="ingester", namespace="mimir", cluster_type="management_cluster"}'
        values: "15+0x20 5+0x40 16+0x140 5+0x40 15+0x60" # mimir-ingester real memory usage gradually increases until it goes beyond 90% of the memory requests.
      - series: 'sum by(namespace) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster"}'
        values: "24+0x300"                                 # mimir-ingester memory requests stay the same for the entire duration of the test.
      - series: 'sum(rate(container_cpu_usage_seconds_total{container="ingester", namespace="mimir", cluster_type="management_cluster"}[5m])) by (namespace)'
        values: "1+0x100 0.6+0x40 1+0x60 0.6+0x40 1+0x60"  # mimir-ingester real cpu usage gradually increases until it goes beyond 90% of the cpu requests.
      - series: 'sum by(namespace) (kube_pod_container_resource_requests{container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster"})'
        values: "3+0x300"                                  # mimir-ingester cpu requests stay the same for the entire duration of the test
    alert_rule_test:
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 15m  # should be OK after 15 minutes
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 55m  # After 55 minutes, should fire an alert
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir/"
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 100m  # After 140m minutes, all should be back to normal
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 135m  # After 55 minutes, should fire an alert
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir/"
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 180m  # After 140m minutes, all should be back to normal
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 235m  # After 55 minutes, should fire an alert
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir/"
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 280m  # After 140m minutes, all should be back to normal
