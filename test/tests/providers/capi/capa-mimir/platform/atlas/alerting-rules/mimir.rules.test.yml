---
rule_files:
  - mimir.rules.yml

tests:
  - interval: 1m
    input_series:
      # For the first 60min: test with 1 pod: up, none, up, down, up
      - series: 'up{job="mimir/ingester", container="ingester"}'
        values: "1+0x60 _x30 1+0x30 0+0x30 1+0x30"
    alert_rule_test:
      - alertname:  Heartbeat
        eval_time: 20m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              opsrecipe: "mimir/"
      - alertname:  Heartbeat
        eval_time: 70m
      - alertname:  Heartbeat
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              opsrecipe: "mimir/"
      - alertname:  Heartbeat
        eval_time: 140m
      - alertname:  Heartbeat
        eval_time: 165m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              opsrecipe: "mimir/"
  - interval: 1m
    input_series:
      # For the first 60min: test with 1 pod: none, up, down
      - series: 'up{job="mimir/ingester", container="ingester", cluster_type="management_cluster", cluster_id="gauss", installation="gauss", provider="aws", pipeline="testing", service="mimir-ingester"}'
        values: "_x20 1+0x20 0+0x20"
    alert_rule_test:
      - alertname:  MimirComponentDown
        eval_time: 10m
      - alertname:  MimirComponentDown
        eval_time: 30m
      - alertname:  MimirComponentDown
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              service: mimir-ingester
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: gauss
              installation: gauss
              provider: aws
              pipeline: testing
            exp_annotations:
              dashboard: ffcd83628d7d4b5a03d1cafd159e6c9c/mimir-overview
              description: "Mimir component : mimir-ingester is down."
              opsrecipe: "mimir/"
  - interval: 1m
    input_series:
      # test with 1 pod: none, up, down
      - series: 'up{job="alloy-rules", cluster_type="management_cluster", cluster_id="golem", provider="capa", pipeline="testing", installation="golem", namespace="mimir"}'
        values: "_x20 1+0x70 0+0x70"
    alert_rule_test:
      - alertname: AlloyForPrometheusRulesDown
        eval_time: 10m
      - alertname: AlloyForPrometheusRulesDown
        eval_time: 80m
      - alertname: AlloyForPrometheusRulesDown
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cluster_id: golem
              installation: golem
              provider: capa
              pipeline: testing
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: "Alloy sending PrometheusRules to Mimir ruler is down."
              opsrecipe: "prometheus-rules/"
  - interval: 1m
    input_series:
      # test: none, rate > 0, rate = 0
      - series: 'mimir_rules_events_failed_total{cluster_type="management_cluster", cluster_id="golem", installation="golem", namespace="mimir"}'
        values: "_x20 1+1x80 0+0x70"
    alert_rule_test:
      - alertname: MimirRulerEventsFailed
        eval_time: 40m
      - alertname: MimirRulerEventsFailed
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cluster_id: golem
              cluster_type: management_cluster
              installation: golem
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: 631e15d5d85afb2ca8e35d62984eeaa0/mimir-ruler
              description: "Mimir ruler is failing to process PrometheusRules."
              opsrecipe: "mimir/"
      - alertname: MimirRulerEventsFailed
        eval_time: 160m
  - interval: 1m
    input_series:
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container="mimir-ingester"}'
        values: "0+0x20 0+5x20 100+0x140" # 0 restarts after 20 minutes then we restart 5 times per minute for 20 minutes then we stop restarting for 140 minutes
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container="prometheus"}'
        values: "0+5x180"                 # prometheus container restarts 5 times per minute for 180 minutes
    alert_rule_test:
      - alertname: MimirRestartingTooOften
        eval_time: 15m  # should be OK after 15 minutes
      - alertname: MimirRestartingTooOften
        eval_time: 85m  # After 85 minutes, should fire an alert for the t+85 error
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: ffcd83628d7d4b5a03d1cafd159e6c9c/mimir-overview
              description: Mimir containers are restarting too often.
              opsrecipe: "mimir/"
      - alertname: MimirRestartingTooOften
        eval_time: 140m  # After 140m minutes, all should be back to normal
  # Test for MimirIngesterNeedsToBeScaledUp alert
  - interval: 1m
    input_series:
      # mimir-ingester real memory usage gradually increases until it goes beyond 90% of the memory requests.
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "8+0x20 11+0x70 8+0x140 11+0x70 8+0x60"
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "8+0x20 11+0x70 8+0x140 11+0x70 8+0x60"
      # mimir-ingester memory requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "12+0x400"
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "12+0x400"
      # mimir-ingester real cpu usage gradually increases until it goes beyond 90% of the cpu requests.                              
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "0+60x100 6000+110x70 10400+60x60 14000+110x70 18400+60x60"
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "0+60x400"
      # mimir-ingester cpu requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "1.5+0x400"                                 
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "1.5+0x400"                                 
    alert_rule_test:
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 15m
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 85m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capa"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming too much resources and needs to be scaled up.
              opsrecipe: "mimir-ingester/"
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 130m
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 170m 
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capa"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming too much resources and needs to be scaled up.
              opsrecipe: "mimir-ingester/"
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 210m
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 295m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capa"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming too much resources and needs to be scaled up.
              opsrecipe: "mimir-ingester/"
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 350m
  # Test for MimirIngesterNeedsToBeScaledDown alert
  - interval: 1m
    input_series:
      # mimir-ingester real memory usage gradually decreases until it goes below 30% of the memory requests.
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "8+0x20 2+0x40 8+0x140 2+0x40 8+0x60"
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "8+0x20 2+0x40 8+0x140 2+0x40 8+0x60"
      # mimir-ingester memory requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "12+0x300"
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "12+0x300"       
      # mimir-ingester real cpu usage gradually increases until it goes below 30% of the cpu requests.                        
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "0+60x100 6000+10x40 6400+60x60 10000+10x40 10400+60x60"
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "0+30x300"
      # mimir-ingester cpu requests stay the same for the entire duration of the test
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "1.5+0x300"                                 
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "1.5+0x300"                                 
    alert_rule_test:
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 15m 
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 55m 
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 100m
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 135m
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 180m 
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 240m 
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capa"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              opsrecipe: "mimir-ingester/"
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 280m 
  # Test for MimirHPAReachedMaxReplicas alert
  - interval: 1m
    input_series:
      # HPA max replicas = 3 for the whole test
      # HPA target metric = 90% for the whole test
      # Cases:
      #   desired_replicas < max_replicas AND current_utilization < target_utilization does not fire
      #   desired_replicas < max_replicas AND current_utilization = target_utilization does not fire
      #   desired_replicas < max_replicas AND current_utilization > target_utilization does not fire
      #   desired_replicas = max_replicas AND current_utilization < target_utilization does not fire
      #   desired_replicas = max_replicas AND current_utilization = target_utilization does not fire
      #   desired_replicas = max_replicas AND current_utilization > target_utilization does fire
      #   desired_replicas > max_replicas AND current_utilization < target_utilization does not fire
      #   desired_replicas > max_replicas AND current_utilization = target_utilization does not fire
      #   desired_replicas > max_replicas AND current_utilization > target_utilization does fire
      - series: 'kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="mimir-distributor", namespace="mimir"}'
        values: '3+0x360'
      - series: 'kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler="mimir-distributor", namespace="mimir"}'
        values: '2+0x120 3+0x120 4+0x120'
      - series: 'kube_horizontalpodautoscaler_spec_target_metric{horizontalpodautoscaler="mimir-distributor", namespace="mimir", metric_name="cpu", metric_target_type="utilization"}'
        values: '90+0x360'
      # HPA current metric = 80% for 10mn, then increase to 90% for 10mn
      - series: 'kube_horizontalpodautoscaler_status_target_metric{horizontalpodautoscaler="mimir-distributor", namespace="mimir", metric_name="cpu", metric_target_type="utilization"}'
        values: '80+0x40 90+0x40 100+0x40 80+0x40 90+0x40 100+0x40 80+0x40 90+0x40 100+0x40'
    alert_rule_test:
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 234m
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 235m
        exp_alerts:
          -  exp_labels:
               area: platform
               cancel_if_cluster_status_creating: "true"
               cancel_if_cluster_status_deleting: "true"
               cancel_if_cluster_status_updating: "true"
               cancel_if_outside_working_hours: "true"
               severity: page
               team: atlas
               topic: observability
               horizontalpodautoscaler: mimir-distributor
               namespace: mimir
             exp_annotations:
               description: "Mimir ${ labels.horizontalpodautoscaler } HPA has reached maximum replicas and consume too much resources, it needs to be scaled up."
               opsrecipe: "mimir-hpa/"
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 246m
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 360m
        exp_alerts:
          -  exp_labels:
               area: platform
               cancel_if_cluster_status_creating: "true"
               cancel_if_cluster_status_deleting: "true"
               cancel_if_cluster_status_updating: "true"
               cancel_if_outside_working_hours: "true"
               severity: page
               team: atlas
               topic: observability
               horizontalpodautoscaler: mimir-distributor
               namespace: mimir
             exp_annotations:
               description: "Mimir ${ labels.horizontalpodautoscaler } HPA has reached maximum replicas and consume too much resources, it needs to be scaled up."
               opsrecipe: "mimir-hpa/"
  # Test for MimirCompactorFailedCompaction alert
  - interval: 1m
    input_series:
      # mimir-ingester real memory usage gradually decreases until it goes below 30% of the memory requests.
      - series: 'cortex_compactor_runs_failed_total{reason="error", installation="golem", cluster_id="golem", namespace="mimir", pipeline="testing", provider="capa"}'
        values: "8+0x20 1+0x40 0+0x20 4+0x130 0+0x190"                             
    alert_rule_test:
      - alertname: MimirCompactorFailedCompaction
        eval_time: 15m 
      - alertname: MimirCompactorFailedCompaction
        eval_time: 55m 
      - alertname: MimirCompactorFailedCompaction
        eval_time: 120m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capa"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: 09a5c49e9cdb2f2b24c6d184574a07fd/mimir-compactor-resources
              description: Mimir compactor has been failing its compactions for 2 hours.
              opsrecipe: "mimir/"
      - alertname: MimirCompactorFailedCompaction
        eval_time: 205m 
      - alertname: MimirCompactorFailedCompaction
        eval_time: 350m
