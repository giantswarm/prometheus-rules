---
rule_files:
  - alertmanager.rules.yml

tests:
  - interval: 1m
    input_series:
      # after 1h, slack notification fails during 2h then works again (15m group_interval) => alert fires after 3 successive failures
      - series: 'alertmanager_notifications_failed_total{integration="slack", cluster_type="management_cluster"}'
        values: "0x60 1+0x15 2+0x15 3+0x15 4+0x15 5+0x15 6+0x15 7+0x15 8+0x15 8+0x120"
      # after 1h, slack notification fails 2 times during 30mn than works again => alert must not fires
      - series: 'alertmanager_notifications_failed_total{integration="webhook", cluster_type="management_cluster"}'
        values: "0x60 1+0x15 2+0x15 2+0x15 2+0x15 2+0x15 2+0x15 2+0x15 2+0x15 2+0x120"
    alert_rule_test:
      - alertname:  AlertmanagerNotifyNotificationsFailing
        eval_time: 10m
      - alertname:  AlertmanagerNotifyNotificationsFailing
        eval_time: 90m
      - alertname:  AlertmanagerNotifyNotificationsFailing
        eval_time: 95m
      - alertname:  AlertmanagerNotifyNotificationsFailing
        eval_time: 106m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              integration: slack
              severity: page
              team: atlas
              topic: monitoring
            exp_annotations:
              __dashboardUid__: alertmanager-overview
              dashboardQueryParams: "orgId=1"
              description: "AlertManager slack notifications are failing."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/alert-manager-notifications-failing/
      - alertname:  AlertmanagerNotifyNotificationsFailing
        eval_time: 240m
  - interval: 1m
    input_series:
      # after 1h, opsgenie notification fails during 45m then works again for 1h, finally fails 1 time (group_interval=15m)
      # => alert fires after 2 successive failures only
      - series: 'alertmanager_notifications_failed_total{integration="opsgenie", cluster_type="management_cluster"}'
        values: "0x60 1+0x15 2+0x15 2+0x15 2+0x60 3+0x15 3+0x60"
    alert_rule_test:
      - alertname:  AlertmanagerPageNotificationsFailing
        eval_time: 10m
      - alertname:  AlertmanagerPageNotificationsFailing
        eval_time: 75m
      - alertname:  AlertmanagerPageNotificationsFailing
        eval_time: 91m
        exp_alerts:
          - exp_labels:
              area: platform
              cluster_type: management_cluster
              integration: opsgenie
              severity: notify
              team: atlas
              topic: monitoring
            exp_annotations:
              __dashboardUid__: alertmanager-overview
              dashboardQueryParams: "orgId=1"
              description: "AlertManager opsgenie notifications are failing."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/alert-manager-notifications-failing/
      - alertname:  AlertmanagerPageNotificationsFailing
        eval_time: 180m
      - alertname:  AlertmanagerPageNotificationsFailing
        eval_time: 210m
