---
rule_files:
- grafana-cloud.rules.yml

tests:
  # Tests for `MimirToGrafanaCloudExporterDown` alert
  - interval: 1m
    input_series:
      - series: 'up{job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", cluster_type="management_cluster", installation="myinstall", namespace="mimir", customer="giantswarm", pipeline="stable", provider="capa", region="eu-west-2"}'
        values: "_x60 1+0x60 0+0x60 1+0x60"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              installation: myinstall
              job: mimir/mimir-to-grafana-cloud
              pipeline: stable
              provider: capa
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: iWowmlSmk
              dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
              description: "Prometheus Mimir to Grafana-Cloud is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              customer: giantswarm
              installation: myinstall
              job: mimir/mimir-to-grafana-cloud
              namespace: mimir
              pipeline: stable
              provider: capa
              region: eu-west-2
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: iWowmlSmk
              dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
              description: "Prometheus Mimir to Grafana-Cloud is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 200m
  # Tests for `MimirToGrafanaCloudExporterFailures` alert
  - interval: 1m
    input_series:
      # remote read is working for 2 hours and then fails for 1 hour
      - series: 'prometheus_remote_storage_read_queries_total{code="200", job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x60 0+10x60 0+0x60 0+10x180"
      # remote write has no failure for 4 hours and then fails for 2 hours
      - series: 'prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x60 0+0x180 0+10x120"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is failing to read or write data."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 200m
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 280m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is failing to read or write data."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
  # Tests for `MimirToGrafanaCloudExporterTooManyRestarts` alert
  - interval: 1m
    input_series:
      # remote read is working for 2 hours and then fails for 1 hour
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea5", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x60 1+0x60 _x80"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea6", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x122 1+0x2 _x78"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea7", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x124 1+0x2 _x76"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea8", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x126 1+0x2 _x74"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea9", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x128 1+0x72"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 140m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              pod: "prometheus-mimir-to-grafana-cloud-0"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is restarting too much."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 180m
  # Tests for `IncorrectResourceUsageData` alert
  # This alert compare metrics for CPU and memory between node-exporter (aggregation:node:*) and kubelet (machine_*).
  # This alert is triggered when the values from node-exporter are 10% lower than the values from kubelet.
  # Test cases:
  # - same value yield no alert
  # - lower value from node-exporter yield no alert
  # - incorrect value from node-exporter for 20m yield no alert
  # - higher value from node-exporter yield no alert
  # The tests use 180m intervals since the alert uses a 2h time window and wait for 1h before firing
  # The tests are split in 2 parts: one for CPU and one for memory with the same test cases
  - interval: 1m
    input_series:
      # First test CPU
      - series: 'aggregation:node:cpu_cores_total{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1"}'
        values: '64x180 48x180 64x180  0x20 64x160 128x180'
      - series: 'machine_cpu_cores{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-a"}'
        values: '16x900'
      - series: 'machine_cpu_cores{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-b"}'
        values: '16x900'
      - series: 'machine_cpu_cores{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-c"}'
        values: '16x900'
      - series: 'machine_cpu_cores{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-d"}'
        values: '16x900'

      # Then test memory with a 900mn padding
      - series: 'aggregation:node:memory_memtotal_bytes_total{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1"}'
        values: '256x900 256x180 128x180 256x180 0x20 256x160 512x180'
      - series: 'machine_memory_bytes{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-a"}'
        values: '64x1800'
      - series: 'machine_memory_bytes{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-b"}'
        values: '64x1800'
      - series: 'machine_memory_bytes{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-c"}'
        values: '64x1800'
      - series: 'machine_memory_bytes{cluster_id="mycluster", customer="giantswarm", installation="myinstall", pipeline="testing", provider="capa", region="eu-west-1", node="node-d"}'
        values: '64x1800'
    alert_rule_test:
      # Test case 1 (cpu): no alert
      - alertname: IncorrectResourceUsageData
        eval_time: 181m
      # Test case 2 (cpu): alert fire after 3h
      - alertname: IncorrectResourceUsageData
        eval_time: 361m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "mycluster"
              customer: "giantswarm"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
              region: "eu-west-1"
            exp_annotations:
              description: 'Data used in the Grafana Cloud Resource Usage dashboard is incorrect for cluster mycluster.'
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/resource-usage-dashboard/
      # Test case 3 (cpu): incorrect value for 20m yield no alert
      - alertname: IncorrectResourceUsageData
        eval_time: 721m
      # Test case 4 (cpu): higher value from node-exporter yield no alert
      - alertname: IncorrectResourceUsageData
        eval_time: 901m

      # Test case 1 (memory): no alert
      - alertname: IncorrectResourceUsageData
        eval_time: 1081m
      # Test case 2 (memory): alert fire after 3h
      - alertname: IncorrectResourceUsageData
        eval_time: 1261m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "mycluster"
              customer: "giantswarm"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
              region: "eu-west-1"
            exp_annotations:
              description: 'Data used in the Grafana Cloud Resource Usage dashboard is incorrect for cluster mycluster.'
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/resource-usage-dashboard/
      # Test case 3 (memory): incorrect value for 20m yield no alert
      - alertname: IncorrectResourceUsageData
        eval_time: 1620m
      # Test case 4 (memory): higher value from node-exporter yield no alert
      - alertname: IncorrectResourceUsageData
        eval_time: 1801m
