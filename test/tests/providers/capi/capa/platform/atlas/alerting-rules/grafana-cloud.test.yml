---
rule_files:
- grafana-cloud.rules.yml

tests:
  # Tests for `MimirToGrafanaCloudExporterDown` alert
  - interval: 1m
    input_series:
      - series: 'up{job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", cluster_type="management_cluster", installation="myinstall", namespace="mimir", customer="giantswarm", pipeline="stable", provider="capa", region="eu-west-2"}'
        values: "_x60 1+0x60 0+0x60 1+0x60"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              installation: myinstall
              job: mimir/mimir-to-grafana-cloud
              pipeline: stable
              provider: capa
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: iWowmlSmk
              dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
              description: "Prometheus Mimir to Grafana-Cloud is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              customer: giantswarm
              installation: myinstall
              job: mimir/mimir-to-grafana-cloud
              namespace: mimir
              pipeline: stable
              provider: capa
              region: eu-west-2
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: iWowmlSmk
              dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
              description: "Prometheus Mimir to Grafana-Cloud is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 200m
  # Tests for `MimirToGrafanaCloudExporterFailures` alert
  - interval: 1m
    input_series:
      # remote read is working for 2 hours and then fails for 1 hour
      - series: 'prometheus_remote_storage_read_queries_total{code="200", job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x60 0+10x60 0+0x60 0+10x180"
      # remote write has no failure for 4 hours and then fails for 2 hours
      - series: 'prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x60 0+0x180 0+10x120"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is failing to read or write data."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 200m
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 280m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is failing to read or write data."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
  # Tests for `MimirToGrafanaCloudExporterTooManyRestarts` alert
  - interval: 1m
    input_series:
      # remote read is working for 2 hours and then fails for 1 hour
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea5", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x60 1+0x60 _x80"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea6", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x122 1+0x2 _x78"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea7", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x124 1+0x2 _x76"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea8", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x126 1+0x2 _x74"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-8e08b9c64ea9", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "_x128 1+0x72"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 140m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              pod: "prometheus-mimir-to-grafana-cloud-0"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "capa"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is restarting too much."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 180m
