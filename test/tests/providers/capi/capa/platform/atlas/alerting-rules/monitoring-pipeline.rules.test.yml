---
rule_files:
  - monitoring-pipeline.rules.yml

# Setting evaluation interval to 1h
# to make it faster on long test duration.
evaluation_interval: 1h

tests:
  # Test JobScrapingFailure and CriticalJobScrapingFailure
  - interval: 1h
    input_series:
      - series: 'up{job="apiserver", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      # critcal target up for 5d and down for 5d
      - series: 'up{job="kube-controller-manager", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x120 0+0x120"
      - series: 'up{job="kube-scheduler", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      - series: 'up{job="kubelet", metrics_path="/metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      - series: 'up{job="node-exporter", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      - series: 'up{job="kube-state-metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      # Add bastion host test to ensure we do not page
      - series: 'up{job="node-exporter", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      # non-critcal target up for 5d and down for 5d
      - series: 'up{job="app-exporter", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x120 0+0x120"
    alert_rule_test:
      - alertname: CriticalJobScrapingFailure
        eval_time: 30m
      - alertname: JobScrapingFailure
        eval_time: 1d
      - alertname: CriticalJobScrapingFailure
        eval_time: 4d
      # This alert fires for both critical and non-critical targets
      - alertname: JobScrapingFailure
        eval_time: 7d
        exp_alerts:
          - exp_labels:
              area: platform
              severity: notify
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "golem"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "kube-controller-manager"
            exp_annotations:
              __dashboardUid__: servicemonitors-details
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
              summary: "Monitoring agent failed to scrape all targets in a job."
              description: "Monitoring agents for cluster golem/golem has failed to scrape all targets in kube-controller-manager job."
          - exp_labels:
              area: platform
              severity: notify
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "golem"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "app-exporter"
            exp_annotations:
              __dashboardUid__: servicemonitors-details
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
              summary: "Monitoring agent failed to scrape all targets in a job."
              description: "Monitoring agents for cluster golem/golem has failed to scrape all targets in app-exporter job."
  
      # This fires only for critical target down.
      - alertname: CriticalJobScrapingFailure
        eval_time: 9d
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cluster_id: "golem"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "kube-controller-manager"
              cancel_if_outside_working_hours: "true"
            exp_annotations:
              __dashboardUid__: servicemonitors-details
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-job-scraping-failure/
              summary: "Monitoring agent failed to scrape all targets in a job."
              description: "Monitoring agents for cluster golem/golem has failed to scrape all targets in kube-controller-manager job."


  # Test MetricForwardingErrors
  - interval: 1m
    input_series:
      # Test case where alert should not fire (error rate < 1%)
      - series: 'prometheus_remote_storage_samples_failed_total{remote_name="test-remote", url="http://example.com"}'
        values: '0+0x65 1+1x60' # Minimal failures for 2h+ period
      - series: 'prometheus_remote_storage_samples_total{remote_name="test-remote", url="http://example.com"}'
        values: '1000+1000x125' # Normal sample count

      # Test case where alert should fire (error rate > 1%)
      - series: 'prometheus_remote_storage_samples_failed_total{remote_name="error-remote", url="http://error.com"}'
        values: '0+0x5 50+50x120' # Significant failures over time
      - series: 'prometheus_remote_storage_samples_total{remote_name="error-remote", url="http://error.com"}'
        values: '1000+1000x125' # Normal sample count

    alert_rule_test:
      # Alert shouldn't fire for low error rate
      - eval_time: 65m
        alertname: MetricForwardingErrors
        exp_alerts: []
      
      # Alert shouldn't fire yet (not enough time elapsed)
      - eval_time: 70m
        alertname: MetricForwardingErrors
        exp_alerts: []
      
      # Alert should fire after 1h of high error rate
      - eval_time: 125m
        alertname: MetricForwardingErrors
        exp_alerts:
          - exp_labels:
              alertname: MetricForwardingErrors
              remote_name: error-remote
              url: http://error.com
              area: platform
              cancel_if_outside_working_hours: "true"
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              summary: Monitoring agent fails to send samples to remote storage.
              description: Monitoring agent failed to send 4.8% of the samples to error-remote:http://error.com.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/monitoring-pipeline/
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
