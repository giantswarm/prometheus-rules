---
rule_files:
  - mimir.rules.yml

tests:
  - interval: 1m
    input_series:
      # For the first 60min: test with 1 pod: up, none, up, down, up
      - series: 'up{job="mimir/ingester", container="ingester"}'
        values: "1+0x60 _x30 1+0x30 0+0x30 1+0x30"
    alert_rule_test:
      - alertname:  Heartbeat
        eval_time: 20m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname:  Heartbeat
        eval_time: 70m
      - alertname:  Heartbeat
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname:  Heartbeat
        eval_time: 140m
      - alertname:  Heartbeat
        eval_time: 165m
        exp_alerts:
          - exp_labels:
              area: platform
              job: mimir/ingester
              container: ingester
              installation: myinstall
              team: atlas
              topic: observability
              type: mimir-heartbeat
            exp_annotations:
              description: "This alert is used to ensure the entire alerting pipeline is functional."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
  - interval: 1m
    input_series:
      # For the first 60min: test with 1 pod: none, up, down
      - series: 'up{job="mimir/ingester", container="ingester", cluster_type="management_cluster", cluster_id="gauss", installation="gauss", provider="aws", pipeline="testing", service="mimir-ingester"}'
        values: "_x20 1+0x20 0+0x20"
    alert_rule_test:
      - alertname:  MimirComponentDown
        eval_time: 10m
      - alertname:  MimirComponentDown
        eval_time: 30m
      - alertname:  MimirComponentDown
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              service: mimir-ingester
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: gauss
              installation: gauss
              provider: aws
              pipeline: testing
            exp_annotations:
              __dashboardUid__: ffcd83628d7d4b5a03d1cafd159e6c9c
              dashboardQueryParams: "orgId=2"
              description: "Mimir component : mimir-ingester is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
  - interval: 1m
    input_series:
      # test: none, rate > 0, rate = 0
      - series: 'mimir_rules_events_failed_total{cluster_type="management_cluster", cluster_id="golem", installation="golem", namespace="mimir"}'
        values: "_x20 1+1x80 0+0x70"
    alert_rule_test:
      - alertname: MimirRulerEventsFailed
        eval_time: 40m
      - alertname: MimirRulerEventsFailed
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              cluster_type: management_cluster
              installation: golem
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: 631e15d5d85afb2ca8e35d62984eeaa0
              dashboardQueryParams: "orgId=2"
              description: "Mimir ruler is failing to process PrometheusRules."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname: MimirRulerEventsFailed
        eval_time: 160m
  - interval: 1m
    input_series:
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container="mimir-ingester"}'
        values: "0+0x20 0+5x20 100+0x140" # 0 restarts after 20 minutes then we restart 5 times per minute for 20 minutes then we stop restarting for 140 minutes
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="mimir", container="prometheus"}'
        values: "0+5x180"                 # prometheus container restarts 5 times per minute for 180 minutes
    alert_rule_test:
      - alertname: MimirRestartingTooOften
        eval_time: 15m  # should be OK after 15 minutes
      - alertname: MimirRestartingTooOften
        eval_time: 85m  # After 85 minutes, should fire an alert for the t+85 error
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              container: mimir-ingester
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: ffcd83628d7d4b5a03d1cafd159e6c9c
              dashboardQueryParams: "orgId=2"
              description: Mimir containers are restarting too often.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname: MimirRestartingTooOften
        eval_time: 140m  # After 140m minutes, all should be back to normal
  # Test for MimirIngesterNeedsToBeScaledUp alert
  - interval: 1m
    input_series:
      # mimir-ingester real memory usage gradually increases until it goes beyond 90% of the memory requests.
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "8+0x20 11+0x70 8+0x140 11+0x70 8+0x60"
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "8+0x20 11+0x70 8+0x140 11+0x70 8+0x60"
      # mimir-ingester memory requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "12+0x400"
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "12+0x400"
      # mimir-ingester real cpu usage gradually increases until it goes beyond 90% of the cpu requests.                              
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "0+60x100 6000+110x70 10400+60x60 14000+110x70 18400+60x60"
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "0+60x400"
      # mimir-ingester cpu requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "1.5+0x400"                                 
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "1.5+0x400"                                 
    alert_rule_test:
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 15m
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 85m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capz"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming too much resources and needs to be scaled up.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-ingester/
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 130m
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 170m 
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capz"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming too much resources and needs to be scaled up.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-ingester/
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 210m
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 295m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capz"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming too much resources and needs to be scaled up.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-ingester/
      - alertname: MimirIngesterNeedsToBeScaledUp
        eval_time: 350m
  # Test for MimirIngesterNeedsToBeScaledDown alert
  - interval: 1m
    input_series:
      # mimir-ingester real memory usage gradually decreases until it goes below 30% of the memory requests.
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "8+0x20 2+0x40 8+0x140 2+0x40 8+0x60"
      - series: 'container_memory_working_set_bytes{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "8+0x20 2+0x40 8+0x140 2+0x40 8+0x60"
      # mimir-ingester memory requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "12+0x300"
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "12+0x300"       
      # mimir-ingester real cpu usage gradually increases until it goes below 30% of the cpu requests.                        
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-0", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "0+60x100 6000+10x40 6400+60x60 10000+10x40 10400+60x60"
      - series: 'container_cpu_usage_seconds_total{pod="mimir-ingester-1", container="ingester", namespace="mimir", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "0+30x300"
      # mimir-ingester cpu requests stay the same for the entire duration of the test
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-0", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "1.5+0x300"                                 
      - series: 'kube_pod_container_resource_requests{pod="mimir-ingester-1", container="ingester", namespace="mimir", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capz", region="eu-west-2"}'
        values: "1.5+0x300"                                 
    alert_rule_test:
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 15m 
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 55m 
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 100m
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 135m
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 180m 
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 240m 
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capz"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Mimir ingester is consuming very few resources and needs to be scaled down.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-ingester/
      - alertname: MimirIngesterNeedsToBeScaledDown
        eval_time: 280m 
  # Test for MimirHPAReachedMaxReplicas alert
  - interval: 1m
    input_series:
      # HPA max replicas = 3 for the whole test
      # HPA target metric = 90% for the whole test
      # Cases:
      #   desired_replicas < max_replicas AND current_utilization < target_utilization does not fire
      #   desired_replicas < max_replicas AND current_utilization = target_utilization does not fire
      #   desired_replicas < max_replicas AND current_utilization > target_utilization does not fire
      #   desired_replicas = max_replicas AND current_utilization < target_utilization does not fire
      #   desired_replicas = max_replicas AND current_utilization = target_utilization does not fire
      #   desired_replicas = max_replicas AND current_utilization > target_utilization does fire
      #   desired_replicas > max_replicas AND current_utilization < target_utilization does not fire
      #   desired_replicas > max_replicas AND current_utilization = target_utilization does not fire
      #   desired_replicas > max_replicas AND current_utilization > target_utilization does fire
      - series: 'kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="mimir-distributor", namespace="mimir"}'
        values: '3+0x360'
      - series: 'kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler="mimir-distributor", namespace="mimir"}'
        values: '2+0x120 3+0x120 4+0x120'
      - series: 'kube_horizontalpodautoscaler_spec_target_metric{horizontalpodautoscaler="mimir-distributor", namespace="mimir", metric_name="cpu", metric_target_type="utilization"}'
        values: '90+0x360'
      # HPA current metric = 80% for 10mn, then increase to 90% for 10mn
      - series: 'kube_horizontalpodautoscaler_status_target_metric{horizontalpodautoscaler="mimir-distributor", namespace="mimir", metric_name="cpu", metric_target_type="utilization"}'
        values: '80+0x40 90+0x40 100+0x40 80+0x40 90+0x40 100+0x40 80+0x40 90+0x40 100+0x40'
    alert_rule_test:
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 234m
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 235m
        exp_alerts:
          -  exp_labels:
               area: platform
               cancel_if_outside_working_hours: "true"
               severity: page
               team: atlas
               topic: observability
               horizontalpodautoscaler: mimir-distributor
               namespace: mimir
             exp_annotations:
               description: "Mimir mimir-distributor HPA has reached maximum replicas and consume too much resources, it needs to be scaled up."
               runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-hpa/
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 246m
      - alertname: MimirHPAReachedMaxReplicas
        eval_time: 360m
        exp_alerts:
          -  exp_labels:
               area: platform
               cancel_if_outside_working_hours: "true"
               severity: page
               team: atlas
               topic: observability
               horizontalpodautoscaler: mimir-distributor
               namespace: mimir
             exp_annotations:
               description: "Mimir mimir-distributor HPA has reached maximum replicas and consume too much resources, it needs to be scaled up."
               runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-hpa/
  # Test for MimirCompactorFailedCompaction alert
  - interval: 1m
    input_series:
      - series: 'cortex_compactor_runs_failed_total{reason="error", installation="golem", cluster_id="golem", namespace="mimir", pipeline="testing", provider="capz"}'
        values: "8+0x20 1+0x40 0+0x20 4+0x130 0+0x190"
      - series: 'cortex_compactor_last_successful_run_timestamp_seconds{installation="golem", cluster_id="golem", namespace="mimir", pipeline="testing", provider="capz"}'
        # No compactions for 2 hours, then 1 successful one at t+3h, another one at t+4h, then 2 more hours with no successful compaction.
        values: '0+0x240 14400+0x60 18000x60 21600+0x240'
    alert_rule_test:
      - alertname: MimirCompactorFailedCompaction
        eval_time: 60m 
      - alertname: MimirCompactorFailedCompaction
        eval_time: 130m 
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capz"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: 09a5c49e9cdb2f2b24c6d184574a07fd
              dashboardQueryParams: "orgId=2"
              description: Mimir compactor has been failing compactions for more than 2 hours since start-up.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir#mimircompactorfailedcompaction
      - alertname: MimirCompactorFailedCompaction
        eval_time: 250m
      - alertname: MimirCompactorFailedCompaction
        eval_time: 480m 
      - alertname: MimirCompactorFailedCompaction
        eval_time: 600m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: "golem"
              pipeline: "testing"
              provider: "capz"
              namespace: mimir
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: 09a5c49e9cdb2f2b24c6d184574a07fd
              dashboardQueryParams: "orgId=2"
              description: Mimir compactor has been failing its compactions for 2 hours.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir#mimircompactorfailedcompaction

  # Test for MimirContinuousTestFailingOnWrites alert
  - interval: 1m
    input_series:
      # Test: none, rate > 0, rate = 0
      - series: 'mimir_continuous_test_writes_failed_total{cluster_id="golem", test="continuous-test", installation="golem", namespace="mimir", pipeline="testing", provider="capz"}'
        values: "_x20 1+1x80 0+0x70"
    alert_rule_test:
      - alertname: MimirContinuousTestFailingOnWrites
        eval_time: 40m
      - alertname: MimirContinuousTestFailingOnWrites
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: golem
              namespace: mimir
              pipeline: testing
              provider: capz
              severity: page
              team: atlas
              test: continuous-test
              topic: observability
            exp_annotations:
              __dashboardUid__: mimir-continuous-test
              dashboardQueryParams: "orgId=2"
              description: "Mimir continuous test continuous-test in golem/mimir is not effectively running because writes are failing."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname: MimirContinuousTestFailingOnWrites
        eval_time: 160m

  # Test for MimirContinuousTestFailingOnReads alert
  - interval: 1m
    input_series:
      # Test: none, rate > 0, rate = 0
      - series: 'mimir_continuous_test_queries_failed_total{cluster_id="golem", test="continuous-test", installation="golem", namespace="mimir", pipeline="testing", provider="capz"}'
        values: "_x20 1+1x80 0+0x70"
    alert_rule_test:
      - alertname: MimirContinuousTestFailingOnReads
        eval_time: 40m
      - alertname: MimirContinuousTestFailingOnReads
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: golem
              namespace: mimir
              pipeline: testing
              provider: capz
              severity: page
              team: atlas
              test: continuous-test
              topic: observability
            exp_annotations:
              __dashboardUid__: mimir-continuous-test
              dashboardQueryParams: "orgId=2"
              description: "Mimir continuous test continuous-test in golem/mimir is not effectively running because queries are failing."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname: MimirContinuousTestFailingOnReads
        eval_time: 160m

  # Test for MimirContinuousTestFailing alert
  - interval: 1m
    input_series:
      # Test: none, rate > 0, rate = 0
      - series: 'mimir_continuous_test_query_result_checks_failed_total{cluster_id="golem", test="continuous-test", installation="golem", namespace="mimir", pipeline="testing", provider="capz"}'
        values: "_x20 1+1x80 0+0x70"
    alert_rule_test:
      - alertname: MimirContinuousTestFailing
        eval_time: 40m
      - alertname: MimirContinuousTestFailing
        eval_time: 95m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: golem
              installation: golem
              namespace: mimir
              pipeline: testing
              provider: capz
              severity: page
              team: atlas
              topic: observability
              test: continuous-test
            exp_annotations:
              __dashboardUid__: mimir-continuous-test
              dashboardQueryParams: "orgId=2"
              description: "Mimir continuous test continuous-test in golem/mimir is not effectively running because queries are failing."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname: MimirContinuousTestFailing
        eval_time: 160m

  # Test for MimirContinuousTestMissing alert
  - interval: 1m
    input_series:
      # Test: none, rate > 0, rate = 0
      - series: 'mimir_continuous_test_writes_total{cluster_id="myinstall", cluster_type="management_cluster", installation="myinstall", namespace="mimir", pipeline="stable", provider="capz"}'
        values: "_x80 1+1x80 0+0x80"
    alert_rule_test:
      - alertname: MimirContinuousTestMissing
        eval_time: 40m
      - alertname: MimirContinuousTestMissing
        eval_time: 70m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              installation: myinstall
              pipeline: stable
              provider: capz
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: mimir-continuous-test
              dashboardQueryParams: "orgId=2"
              description: "Mimir continuous test myinstall is not producing metrics."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/
      - alertname: MimirContinuousTestMissing
        eval_time: 150m
      - alertname: MimirContinuousTestMissing
        eval_time: 230m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              installation: myinstall
              pipeline: stable
              provider: capz
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: mimir-continuous-test
              dashboardQueryParams: "orgId=2"
              description: "Mimir continuous test myinstall is not producing metrics."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/

  # Test for MimirDataPushFailures alert
  - interval: 1m
    input_series:
      - series: 'thanos_objstore_bucket_operation_failures_total{cluster_id="myinstall", cluster_type="management_cluster", component="ingester", operation="upload", installation="myinstall", namespace="mimir", pipeline="stable", provider="capa"}'
        values: "0+10x80 100+0x80"
      - series: 'thanos_objstore_bucket_operations_total{cluster_id="myinstall", cluster_type="management_cluster", component="ingester", operation="upload", installation="myinstall", namespace="mimir", pipeline="stable", provider="capa"}'
        values: "0+100x80 100+0x80"
    alert_rule_test:
      - alertname: MimirDataPushFailures
        eval_time: 40m
      - alertname: MimirDataPushFailures
        eval_time: 70m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              installation: myinstall
              pipeline: stable
              provider: capa
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: e1324ee2a434f4158c00a9ee279d3292
              dashboardQueryParams: "orgId=2"
              description: "Low rate of writes to the Mimir object store."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/#mimirdatapushfailures
      - alertname: MimirDataPushFailures
        eval_time: 140m

  # Test for MimirRulerTooManyFailedQueries alert
  - interval: 1m
    input_series:
      - series: 'cortex_ruler_queries_total{cluster_id="myinstall", installation="myinstall", namespace="mimir", pipeline="stable", pod="mimir-ruler-aaaaaaaaaa-bbbbb", provider="capz"}'
        values: "0x90 0+1x90 90+100x90"
      - series: 'cortex_ruler_queries_failed_total{cluster_id="myinstall", installation="myinstall", namespace="mimir", pipeline="stable", pod="mimir-ruler-aaaaaaaaaa-bbbbb", provider="capz", name="myinstall", type="ControlPlaneReady", status="True"}'
        values: "0x180 0+2x90"
    alert_rule_test:
      - alertname: MimirRulerTooManyFailedQueries
        eval_time: 90m
      - alertname: MimirRulerTooManyFailedQueries
        eval_time: 180m
      - alertname: MimirRulerTooManyFailedQueries
        eval_time: 240m
      - alertname: MimirRulerTooManyFailedQueries
        eval_time: 242m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              installation: myinstall
              namespace: mimir
              pipeline: stable
              pod: mimir-ruler-aaaaaaaaaa-bbbbb
              provider: capz
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: 631e15d5d85afb2ca8e35d62984eeaa0
              dashboardQueryParams: "orgId=2"
              description: "Mimir Ruler mimir-ruler-aaaaaaaaaa-bbbbb is experiencing 2.00% errors while evaluating rules."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir/

  # Test for MimirAlertmanagerSyncConfigsFailing alert
  - interval: 1m
    input_series:
      - series: 'cortex_alertmanager_sync_configs_failed_total{cluster_id="myinstall", installation="myinstall", namespace="mimir", pipeline="stable", pod="mimir-alertmanager-aaaaaaaaaa-bbbbb", provider="capz"}'
        values: "0x90 0+1x90"
    alert_rule_test:
      - alertname: MimirAlertmanagerSyncConfigsFailing
        eval_time: 90m
      - alertname: MimirAlertmanagerSyncConfigsFailing
        eval_time: 125m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              installation: myinstall
              namespace: mimir
              pipeline: stable
              pod: mimir-alertmanager-aaaaaaaaaa-bbbbb
              provider: capz
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: b0d38d318bbddd80476246d4930f9e55
              dashboardQueryParams: "orgId=2"
              description: "Mimir Alertmanager mimir-alertmanager-aaaaaaaaaa-bbbbb is failing to read tenant configurations from storage."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimiralertmanagersyncconfigsfailing

  # Test for MimirAlertmanagerRingCheckFailing alert
  - interval: 1m
    input_series:
      - series: 'cortex_alertmanager_ring_check_errors_total{cluster_id="myinstall", installation="myinstall", namespace="mimir", pipeline="stable", pod="mimir-alertmanager-aaaaaaaaaa-bbbbb", provider="capz"}'
        values: "0x90 0+1x90"
    alert_rule_test:
      - alertname: MimirAlertmanagerRingCheckFailing
        eval_time: 90m
      - alertname: MimirAlertmanagerRingCheckFailing
        eval_time: 105m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              installation: myinstall
              namespace: mimir
              pipeline: stable
              pod: mimir-alertmanager-aaaaaaaaaa-bbbbb
              provider: capz
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: b0d38d318bbddd80476246d4930f9e55
              dashboardQueryParams: "orgId=2"
              description: "Mimir Alertmanager mimir-alertmanager-aaaaaaaaaa-bbbbb is unable to check tenants ownership via the ring."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimiralertmanagerringcheckfailing

  # Test for MimirAlertmanagerReplicationFailing alert
  - interval: 1m
    input_series:
      - series: 'cortex_alertmanager_state_replication_failed_total{cluster_id="myinstall", installation="myinstall", namespace="mimir", pipeline="stable", pod="mimir-alertmanager-aaaaaaaaaa-bbbbb", provider="capz"}'
        values: "0x90 0+1x90"
    alert_rule_test:
      - alertname: MimirAlertmanagerReplicationFailing
        eval_time: 90m
      - alertname: MimirAlertmanagerReplicationFailing
        eval_time: 105m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              installation: myinstall
              namespace: mimir
              pipeline: stable
              pod: mimir-alertmanager-aaaaaaaaaa-bbbbb
              provider: capz
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: b0d38d318bbddd80476246d4930f9e55
              dashboardQueryParams: "orgId=2"
              description: "Mimir Alertmanager mimir-alertmanager-aaaaaaaaaa-bbbbb is failing to replicating partial state to its replicas."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimiralertmanagerreplicationfailing
