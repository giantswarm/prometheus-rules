---
rule_files:
  - alloy.rules.yml

tests:
  # Test AlloySlowComponentEvaluations
  - interval: 1m
    input_series:
      - series: 'alloy_component_evaluation_slow_seconds{cluster_id="gauss", installation="gauss", provider="aws", pipeline="testing", namespace="default", job="alloy-controller", component_id="comp1"}'
        values: "0+0x10 0+1x50 0x50"
    alert_rule_test:
      - alertname: AlloySlowComponentEvaluations
        eval_time: 10m
      - alertname: AlloySlowComponentEvaluations
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cluster_id: gauss
              installation: gauss
              provider: aws
              pipeline: testing
              namespace: default
              job: alloy-controller
              component_id: comp1
              severity: notify
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: bf9f456aad7108b2c808dbd9973e386f/alloy-controller
              description: "Component evaluations are taking too long under job alloy-controller, component_id comp1."
              opsrecipe: "alloy/"
              summary: "Component evaluations are taking too long."
      - alertname: AlloySlowComponentEvaluations
        eval_time: 80m

  # Test AlloyUnhealthyComponents
  - interval: 1m
    input_series:
      - series: 'alloy_component_controller_running_components{health_type="unhealthy", cluster_id="gauss", installation="gauss", provider="aws", pipeline="testing", namespace="default", job="alloy-controller"}'
        values: "0+0x10 1+0x50 0x50"
    alert_rule_test:
      - alertname: AlloyUnhealthyComponents
        eval_time: 10m
      - alertname: AlloyUnhealthyComponents
        eval_time: 30m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cluster_id: gauss
              installation: gauss
              provider: aws
              pipeline: testing
              namespace: default
              job: alloy-controller
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: bf9f456aad7108b2c808dbd9973e386f/alloy-controller
              description: "Unhealthy components detected under job alloy-controller"
              opsrecipe: "alloy/"
              summary: "Unhealthy components detected."
      - alertname: AlloyUnhealthyComponents
        eval_time: 80m

  # Test LoggingAgentDown
  - interval: 1m
    input_series:
      # For the first 80min: test with 1 pod: none, up, down
      - series: 'up{container="alloy", cluster_id="gauss", cluster_type="management_cluster", installation="gauss", job="alloy-logs", pod="alloy-logs-1xxxx", provider="aws", pipeline="testing"}'
        values: "_x20 1+0x20 0+0x40"
      - series: kube_pod_info{cluster_id="gauss", cluster_type="management_cluster", installation="gauss", pod="alloy-logs-1xxxx", node="ip-10-0-5-1.eu-west-1.compute.internal", provider="aws", pipeline="testing"}
        values: "1x180"
      # From 80min: test with 2 pods: 1 up and 1 down, 2 up, 2 down.
      - series: 'up{container="alloy", cluster_id="gauss", cluster_type="management_cluster", installation="gauss", job="alloy-logs", pod="alloy-logs-2xxxx", provider="aws", pipeline="testing"}'
        values: "_x80 1+0x40 1+0x20 0+0x40"
      - series: kube_pod_info{cluster_id="gauss", cluster_type="management_cluster", installation="gauss", pod="alloy-logs-2xxxx", node="ip-10-0-5-2.eu-west-1.compute.internal", provider="aws", pipeline="testing"}
        values: "1x180"
      - series: 'up{container="alloy", cluster_type="management_cluster", cluster_id="gauss", installation="gauss", job="alloy-logs", pod="alloy-logs-3xxxx", provider="aws", pipeline="testing"}'
        values: "_x80 0+0x40 1+0x20 0+0x40"
      - series: kube_pod_info{cluster_id="gauss", cluster_type="management_cluster", installation="gauss", pod="alloy-logs-3xxxx", node="ip-10-0-5-3.eu-west-1.compute.internal", provider="aws", pipeline="testing"}
        values: "1x180"
    alert_rule_test:
      - alertname: LoggingAgentDown
        eval_time: 10m
      - alertname: LoggingAgentDown
        eval_time: 30m
      - alertname: LoggingAgentDown
        eval_time: 71m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_node_unschedulable: "true"
              cancel_if_node_not_ready: "true"
              cluster_id: gauss
              cluster_type: management_cluster
              installation: gauss
              node: ip-10-0-5-1.eu-west-1.compute.internal
              pipeline: testing
              pod: alloy-logs-1xxxx
              provider: aws
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: "53c1ecddc3a1d5d4b8d6cd0c23676c31/alloy-logs-overview"
              description: "Scraping of all logging-agent pods to check if one failed every 30 minutes."
              opsrecipe: "alloy/"
      # Tests with 2 pods
      - alertname: LoggingAgentDown
        eval_time: 111m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_node_unschedulable: "true"
              cancel_if_node_not_ready: "true"
              cluster_id: gauss
              cluster_type: management_cluster
              installation: gauss
              node: ip-10-0-5-3.eu-west-1.compute.internal
              pipeline: testing
              pod: alloy-logs-3xxxx
              provider: aws
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: "53c1ecddc3a1d5d4b8d6cd0c23676c31/alloy-logs-overview"
              description: "Scraping of all logging-agent pods to check if one failed every 30 minutes."
              opsrecipe: "alloy/"
      - alertname: LoggingAgentDown
        eval_time: 121m
      - alertname: LoggingAgentDown
        eval_time: 180m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_node_unschedulable: "true"
              cancel_if_node_not_ready: "true"
              cluster_id: gauss
              cluster_type: management_cluster
              installation: gauss
              node: ip-10-0-5-2.eu-west-1.compute.internal
              pipeline: testing
              pod: alloy-logs-2xxxx
              provider: aws
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: "53c1ecddc3a1d5d4b8d6cd0c23676c31/alloy-logs-overview"
              description: "Scraping of all logging-agent pods to check if one failed every 30 minutes."
              opsrecipe: "alloy/"
          - exp_labels:
              area: platform
              cancel_if_outside_working_hours: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_node_unschedulable: "true"
              cancel_if_node_not_ready: "true"
              cluster_id: gauss
              cluster_type: management_cluster
              installation: gauss
              node: ip-10-0-5-3.eu-west-1.compute.internal
              pipeline: testing
              pod: alloy-logs-3xxxx
              provider: aws
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              dashboard: "53c1ecddc3a1d5d4b8d6cd0c23676c31/alloy-logs-overview"
              description: "Scraping of all logging-agent pods to check if one failed every 30 minutes."
              opsrecipe: "alloy/"
