---
rule_files:
- grafana-cloud.rules.yml

tests:
  # Tests for `MimirToGrafanaCloudExporterDown` alert
  - interval: 1m
    input_series:
      - series: 'up{job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", cluster_type="management_cluster", installation="myinstall", namespace="mimir", customer="giantswarm", pipeline="stable", provider="$provider", region="eu-west-2"}'
        values: "_x60 1+0x60 0+0x60 1+0x60"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 50m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_metrics_broken: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              installation: myinstall
              job: mimir/mimir-to-grafana-cloud
              pipeline: stable
              provider: $provider
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: iWowmlSmk
              dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
              description: "Prometheus Mimir to Grafana-Cloud is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_metrics_broken: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: myinstall
              cluster_type: management_cluster
              customer: giantswarm
              installation: myinstall
              job: mimir/mimir-to-grafana-cloud
              namespace: mimir
              pipeline: stable
              provider: $provider
              region: eu-west-2
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              __dashboardUid__: iWowmlSmk
              dashboardQueryParams: "orgId=1&var-cluster=mimir-to-grafana-cloud"
              description: "Prometheus Mimir to Grafana-Cloud is down."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterDown
        eval_time: 200m
  # Tests for `MimirToGrafanaCloudExporterFailures` alert
  - interval: 1m
    input_series:
      # remote read is working for 2 hours and then fails for 1 hour
      - series: 'prometheus_remote_storage_read_queries_total{code="200", job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x60 0+10x60 0+0x60 0+10x180"
      # remote write has no failure for 4 hours and then fails for 2 hours
      - series: 'prometheus_remote_storage_samples_failed_total{job="mimir/mimir-to-grafana-cloud", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x60 0+0x180 0+10x120"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 160m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "$provider"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is failing to read or write data."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 200m
      - alertname: MimirToGrafanaCloudExporterFailures
        eval_time: 280m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "$provider"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is failing to read or write data."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
  # Tests for `MimirToGrafanaCloudExporterTooManyRestarts` alert
  - interval: 1m
    input_series:
      # prometheus-mimir-to-grafana-cloud starts after 60 minutes, and stays stable for 60 minutes
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000001", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x60 1+0x60 _x120"
      # Every 5 minutes, we get a new pod that stays up for 5 minutes - that lasts for 1 hour
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000002", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x120 1x5 _x115"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000003", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x125 1x5 _x110"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000004", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x130 1x5 _x105"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000005", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x135 1x5 _x100"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000006", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x140 1x5 _x95"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000007", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x145 1x5 _x90"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000008", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x150 1x5 _x85"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000009", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x155 1x5 _x80"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000010", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x160 1x5 _x75"
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000011", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x165 1x5 _x70"
      # Then we have a pod that stays up stable until the end
      - series: 'kube_pod_status_ready{condition="true", uid="0bb4e0cc-12df-4085-8d39-000000000012", pod="prometheus-mimir-to-grafana-cloud-0", cluster_id="myinstall", customer="giantswarm", installation="myinstall", namespace="mimir", pipeline="testing", provider="$provider", region="eu-west-2"}'
        values: "_x170 1x70"
    alert_rule_test:
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 70m
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 140m
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 170m
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cancel_if_outside_working_hours: "true"
              pod: "prometheus-mimir-to-grafana-cloud-0"
              cluster_id: "myinstall"
              installation: "myinstall"
              pipeline: "testing"
              provider: "$provider"
            exp_annotations:
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
              description: "Prometheus Mimir to Grafana-Cloud is restarting too much."
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/ops-recipes/mimir-grafana-cloud-exporter-failing/
      - alertname: MimirToGrafanaCloudExporterTooManyRestarts
        eval_time: 190m
