---
rule_files:
  - loki.rules.yml

tests:
  - interval: 1m
    input_series:
      - series: 'cortex_ring_members{job="loki/loki-read", cluster_id="gaia-wc01", cluster_type="workload_cluster", container="loki", customer="giantswarm", installation="gaia", name="ingester", namespace="loki", node="ip-10-6-2-178.eu-central-1.compute.internal", organization="giantswarm-production", pod="loki-read-b7d9b85d4-xxxxx", provider="aws", pipeline="stable", service_priority="highest", state="Unhealthy"}'
        values: "0+0x20 1+0x160"  # 1 unhealthy value after 20 minutes
      - series: 'cortex_ring_members{job="loki/loki-read", cluster_id="gaia-wc01", cluster_type="workload_cluster", container="loki", customer="giantswarm", installation="gaia", name="ingester", namespace="loki", node="ip-10-6-2-178.eu-central-1.compute.internal", organization="giantswarm-production", pod="loki-read-b7d9b85d4-yyyyy", provider="aws", pipeline="stable", service_priority="highest", state="Unhealthy"}'
        values: "_x30 1+0x10 0+0x60"  # no data for 30 minutes then 1 unhealthy value for 10 minutes and back to normal for 1 hour
      - series: 'loki_panic_total{job="loki/loki-read", cluster_id="gaia-wc01", cluster_type="workload_cluster", container="loki", customer="giantswarm", installation="gaia", namespace="loki", node="ip-10-6-2-178.eu-central-1.compute.internal", organization="giantswarm-production", pod="loki-read-b7d9b85d4-zzzzz", provider="aws", pipeline="stable", service_priority="highest"}'
        values: "0+0x20 1+0x160"  # 1 panic after 20 minutes
      - series: 'loki_request_duration_seconds_count{job="loki/loki-write", cluster_id="gaia-wc01", cluster_type="workload_cluster", container="loki", customer="giantswarm", installation="gaia", instance="10.7.75.90:3100", method="POST", namespace="loki", node="ip-10-6-2-141.eu-central-1.compute.internal", organization="giantswarm-production", pod="loki-write-0", provider="aws", pipeline="stable", route="loki_api_v1_push", service_priority="highest", status_code="204", ws="false"}'
        values: "0+60x180"  # 1 request per second OK for 3 hours
      - series: 'loki_request_duration_seconds_count{job="loki/loki-write", cluster_id="gaia-wc01", cluster_type="workload_cluster", container="loki", customer="giantswarm", installation="gaia", instance="10.7.75.90:3100", method="POST", namespace="loki", node="ip-10-6-2-141.eu-central-1.compute.internal", organization="giantswarm-production", pod="loki-write-0", provider="aws", pipeline="stable", route="loki_api_v1_push", service_priority="highest", status_code="503", ws="false"}'
        values: "0+0x20 0+30x160"  # After 20 minutes, we also have 0.5 rq/s failing
    alert_rule_test:
      - alertname: LokiRequestPanics
        eval_time: 15m  # should be OK after 15 minutes
        exp_alerts:
      - alertname: LokiRequestPanics
        eval_time: 25m  # After 25 minutes, should fire an alert for the t+20 error
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_control_plane_unhealthy: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: gaia-wc01
              installation: gaia
              pipeline: stable
              provider: aws
              job: loki/loki-read
              namespace: loki
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: This alert checks that we have no panic errors on Loki.
              opsrecipe: loki/
      - alertname: LokiRequestPanics
        eval_time: 40m  # After 40 minutes, all should be back to normal
        exp_alerts:

      - alertname: LokiRequestErrors
        eval_time: 15m  # should be OK after 15 minutes
        exp_alerts:
      - alertname: LokiRequestErrors
        eval_time: 160m  # Alert after more than 120m of incident
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_control_plane_unhealthy: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: gaia-wc01
              installation: gaia
              provider: aws
              pipeline: stable
              job: loki/loki-write
              namespace: loki
              route: loki_api_v1_push
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: This alert checks that we have less than 10% errors on Loki requests.
              opsrecipe: loki/

      - alertname: LokiRingUnhealthy
        eval_time: 15m  # should be OK after 15 minutes
        exp_alerts:
      - alertname: LokiRingUnhealthy
        eval_time: 25m  # after 25 minutes we have an unhealthy member, but we want to filter too short events. So no alert yet.
        exp_alerts:
      - alertname: LokiRingUnhealthy
        eval_time: 35m  # special case to validate when a new pod is unhealthy (no data at the beginning)
        exp_alerts:
      - alertname: LokiRingUnhealthy
        eval_time: 60m  # now the event has been there for 20 minutes, we should have an alert.
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_control_plane_unhealthy: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              cluster_id: gaia-wc01
              container: loki
              customer: giantswarm
              installation: gaia
              provider: aws
              pipeline: stable
              job: loki/loki-read
              name: ingester
              namespace: loki
              organization: giantswarm-production
              pod: loki-read-b7d9b85d4-xxxxx
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: "Loki pod loki-read-b7d9b85d4-xxxxx (namespace loki) sees 1 unhealthy ring members"
              opsrecipe: "loki/"
  - interval: 1m
    input_series:
      - series: 'kube_pod_container_status_restarts_total{cluster_type="management_cluster", namespace="loki"}'
        values: "0+0x20 0+5x20 100+0x140" # 0 restarts after 20 minutes then we restart 5 times per minute for 20 minutes then we stop restarting for 140 minutes
    alert_rule_test:
      - alertname: LokiRestartingTooOften
        eval_time: 15m  # should be OK after 15 minutes
        exp_alerts:
      - alertname: LokiRestartingTooOften
        eval_time: 85m  # After 85 minutes, should fire an alert for the t+85 error
        exp_alerts:
          - exp_labels:
              all_pipelines: "true"
              area: platform
              cancel_if_outside_working_hours: "true"
              cluster_type: management_cluster
              namespace: loki
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Loki containers are restarting too often.
              opsrecipe: loki/
      - alertname: LokiRestartingTooOften
        eval_time: 140m  # After 140m minutes, all should be back to normal
        exp_alerts:
  - interval: 1m
    input_series:
      # loki-backend real memory usage gradually decreases until it goes below 30% of the memory requests.
      - series: 'container_memory_working_set_bytes{pod="loki-backend-0", container="loki", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "8+0x20 2+0x1450 8+0x2900 2+0x1450 8+0x1450"
      - series: 'container_memory_working_set_bytes{pod="loki-backend-1", container="loki", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "8+0x20 2+0x1450 8+0x2900 2+0x1450 8+0x1450"
      # loki-backend memory requests stay the same for the entire duration of the test.
      - series: 'kube_pod_container_resource_requests{pod="loki-backend-0", container="loki", namespace="loki", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "12+0x7270"
      - series: 'kube_pod_container_resource_requests{pod="loki-backend-1", container="loki", namespace="loki", unit="byte", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "12+0x7270"       
      # loki-backend real cpu usage gradually increases until it goes below 30% of the cpu requests.                        
      - series: 'container_cpu_usage_seconds_total{pod="loki-backend-0", container="loki", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "0+60x1470 0+10x1450 0+60x1450 0+10x1450 0+60x1450"
      - series: 'container_cpu_usage_seconds_total{pod="loki-backend-1", container="loki", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "0+30x7270"
      # loki-backend cpu requests stay the same for the entire duration of the test
      - series: 'kube_pod_container_resource_requests{pod="loki-backend-0", container="loki", namespace="loki", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "1.5+0x7270"                                 
      - series: 'kube_pod_container_resource_requests{pod="loki-backend-1", container="loki", namespace="loki", unit="core", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "1.5+0x7270"  
    alert_rule_test:
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 15m 
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 55m 
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 100m
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 135m
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 180m 
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 5820m 
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_control_plane_unhealthy: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              installation: "golem"
              cluster_id: "golem"
              labelpod: "loki-backend"
              pipeline: "testing"
              provider: "capa"
              namespace: loki
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              description: Loki component loki-backend is consuming very few resources and needs to be scaled down.
              opsrecipe: "loki/"
      - alertname: LokiNeedsToBeScaledDown
        eval_time: 7265m 
  - interval: 1m
    input_series:
      # loki-backend real memory usage gradually decreases until it goes below 30% of the memory requests.
      - series: 'kube_horizontalpodautoscaler_status_desired_replicas{horizontalpodautoscaler="loki-backend", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "2+0x20 3+0x250 2+0x250"
      - series: 'kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler="loki-backend", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "2+0x520"
      - series: 'kube_horizontalpodautoscaler_status_target_metric{horizontalpodautoscaler="loki-backend", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "60+0x20 120+0x250 60+0x250"
      - series: 'kube_horizontalpodautoscaler_spec_target_metric{horizontalpodautoscaler="loki-backend", namespace="loki", cluster_type="management_cluster", cluster_id="golem", installation="golem", pipeline="testing", provider="capa", region="eu-west-2"}'
        values: "90+0x520"
    alert_rule_test:
      - alertname: LokiHpaReachedMaxReplicas
        eval_time: 15m
      - alertname: LokiHpaReachedMaxReplicas
        eval_time: 265m
        exp_alerts:
          - exp_labels:
              area: platform
              cancel_if_cluster_control_plane_unhealthy: "true"
              cancel_if_cluster_status_creating: "true"
              cancel_if_cluster_status_deleting: "true"
              cancel_if_cluster_status_updating: "true"
              cancel_if_outside_working_hours: "true"
              severity: page
              team: atlas
              topic: observability
              namespace: loki
              horizontalpodautoscaler: loki-backend
              installation: golem
              cluster_id: golem
            exp_annotations:
              description: Loki component loki-backend has reached its maxReplicas number but still needs to be scaled up.
              opsrecipe: loki/
      - alertname: LokiHpaReachedMaxReplicas
        eval_time: 515m 
