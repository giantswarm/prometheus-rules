---
rule_files:
  - monitoring-pipeline.rules.yml

# Setting evaluation interval to 1h
# to make it faster on long test duration.
evaluation_interval: 1h

tests:
  # Test JobScrapingFailure (combined test for both workload and management clusters)
  - interval: 1h
    input_series:
      # Recording rule output for workload cluster app "myapp" - no failures for 1d, then all targets down for 2d
      # Values: 0 for indexes 0-23 (t=0h-23h), then 1 for indexes 24-71 (t=24h-71h)
      - series: 'aggregation:giantswarm:jobscrapingfailures{app="myapp", job="myapp-metrics", installation="golem", cluster_id="mycluster", provider="capa", pipeline="testing", namespace="org-giantswarm", team="team-rocket"}'
        values: "0+0x23 1+0x48"
      # Recording rule output for MC app "prometheus-agent" - no failures for 1d, then all targets down for 2d
      - series: 'aggregation:giantswarm:jobscrapingfailures{app="prometheus-agent", job="prometheus-agent-metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing", namespace="kube-system", team="team-atlas"}'
        values: "0+0x23 1+0x48"
    alert_rule_test:
      # Alert should not fire at 23h (before condition becomes true)
      - alertname: JobScrapingFailure
        eval_time: 23h
        exp_alerts: []
      # Alert should not fire at 24h (condition just became true, but hasn't been true for 1h yet)
      - alertname: JobScrapingFailure
        eval_time: 24h
        exp_alerts: []
      # Alert should fire at 25h (condition has been true for 1h, 'for: 1h' duration satisfied)
      - alertname: JobScrapingFailure
        eval_time: 25h
        exp_alerts:
          # Workload cluster alert
          - exp_labels:
              alertname: JobScrapingFailure
              area: platform
              severity: notify
              topic: observability
              cancel_if_outside_working_hours: "true"
              app: "myapp"
              cluster_id: "mycluster"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "myapp-metrics"
              namespace: "org-giantswarm"
              team: "team-rocket"
            exp_annotations:
              __dashboardUid__: alloy-metrics-targets
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION=golem&CLUSTER=mycluster
              summary: "Monitoring agent failed to scrape all targets for an app / job."
              description: "No targets are reachable for myapp by monitoring agent on cluster golem/mycluster (job: myapp-metrics)"
          # Management cluster alert
          - exp_labels:
              alertname: JobScrapingFailure
              area: platform
              severity: notify
              topic: observability
              cancel_if_outside_working_hours: "true"
              app: "prometheus-agent"
              cluster_id: "golem"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "prometheus-agent-metrics"
              namespace: "kube-system"
              team: "team-atlas"
            exp_annotations:
              __dashboardUid__: alloy-metrics-targets
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION=golem&CLUSTER=golem
              summary: "Monitoring agent failed to scrape all targets for an app / job."
              description: "No targets are reachable for prometheus-agent by monitoring agent on cluster golem/golem (job: prometheus-agent-metrics)"

  # Test MetricForwardingErrors
  - interval: 1m
    input_series:
      # Test case where alert should not fire (error rate < 10%)
      - series: 'prometheus_remote_storage_samples_failed_total{remote_name="test-remote", url="http://example.com", installation="foo", cluster_id="bar"}'
        values: '0+0x65 0+50x60' # Minimal failures for 2h+ period
      - series: 'prometheus_remote_storage_samples_total{remote_name="test-remote", url="http://example.com", installation="foo", cluster_id="bar"}'
        values: '1000+1000x125' # Normal sample count

      # Test case where alert should fire (error rate > 10%)
      - series: 'prometheus_remote_storage_samples_failed_total{remote_name="error-remote", url="http://error.com", installation="foo", cluster_id="bar"}'
        values: '0+0x5 0+120x120' # Significant failures over time
      - series: 'prometheus_remote_storage_samples_total{remote_name="error-remote", url="http://error.com", installation="foo", cluster_id="bar"}'
        values: '1000+1000x125' # Normal sample count

    alert_rule_test:
      # Alert shouldn't fire for low error rate
      - eval_time: 65m
        alertname: MetricForwardingErrors
        exp_alerts: []
      
      # Alert shouldn't fire yet (not enough time elapsed)
      - eval_time: 70m
        alertname: MetricForwardingErrors
        exp_alerts: []
      
      # Alert should fire after 1h of high error rate
      - eval_time: 125m
        alertname: MetricForwardingErrors
        exp_alerts:
          - exp_labels:
              alertname: MetricForwardingErrors
              cluster_id: "bar"
              installation: "foo"
              remote_name: error-remote
              url: http://error.com
              area: platform
              cancel_if_outside_working_hours: "true"
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              summary: Monitoring agent fails to send samples to remote storage.
              description: Monitoring agent failed to send 10.7% of the samples to error-remote:http://error.com.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/monitoring-pipeline/?INSTALLATION=foo&CLUSTER=bar
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
