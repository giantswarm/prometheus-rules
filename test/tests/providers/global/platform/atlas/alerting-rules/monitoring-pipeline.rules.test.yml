---
rule_files:
  - monitoring-pipeline.rules.yml

# Setting evaluation interval to 1h
# to make it faster on long test duration.
evaluation_interval: 1h

tests:
  # Test JobScrapingFailure (for apps in org-* namespaces)
  - interval: 1h
    input_series:
      # App "myapp" in org namespace (workload cluster) - up for 1d, then down for 2d
      - series: 'up{app="myapp", job="myapp-metrics", installation="golem", cluster_id="mycluster", provider="capa", pipeline="testing"}'
        values: "1+0x24 0+0x48"
      # App operator info for myapp in org namespace with team label
      # Note: name must match what label_join creates (cluster_id-app = "mycluster-myapp")
      - series: 'app_operator_app_info{name="mycluster-myapp", namespace="org-giantswarm", team="team-rocket", installation="golem", cluster_id="mycluster"}'
        values: "1+0x72"
      # Another app "otherapp" in org namespace - stays up
      - series: 'up{app="otherapp", job="otherapp-metrics", installation="golem", cluster_id="mycluster", provider="capa", pipeline="testing"}'
        values: "1+0x72"
      # App operator info for otherapp
      - series: 'app_operator_app_info{name="mycluster-otherapp", namespace="org-acme", team="team-phoenix", installation="golem", cluster_id="mycluster"}'
        values: "1+0x72"
    alert_rule_test:
      # Alert should not fire before the service is down for 1d
      - alertname: JobScrapingFailure
        eval_time: 40h
        exp_alerts: []
      # Alert should fire after being down for 1d (downtime starts at 25h, fires at 49h)
      - alertname: JobScrapingFailure
        eval_time: 49h
        exp_alerts:
          - exp_labels:
              area: platform
              severity: notify
              topic: observability
              cancel_if_outside_working_hours: "true"
              app: "mycluster-myapp"
              cluster_id: "mycluster"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "myapp-metrics"
              team: "team-rocket"
            exp_annotations:
              __dashboardUid__: alloy-metrics-targets
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION=golem&CLUSTER=mycluster
              summary: "Monitoring agent failed to scrape all targets for an app / job."
              description: "No targets are reachable for mycluster-myapp by monitoring agent on cluster golem/mycluster (job: myapp-metrics)"

  # Test JobScrapingFailureMC (for apps in non org-* namespaces - management clusters)
  - interval: 1h
    input_series:
      # App "prometheus-agent" in kube-system namespace (management cluster) - up for 1d, then down for 2d
      - series: 'up{app="prometheus-agent", job="prometheus-agent-metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x24 0+0x48"
      # App operator info for prometheus-agent in non-org namespace with team label
      - series: 'app_operator_app_info{name="prometheus-agent", namespace="kube-system", team="team-atlas", installation="golem", cluster_id="golem"}'
        values: "1+0x72"
      # Another app "alloy" in giantswarm namespace - stays up
      - series: 'up{app="alloy", job="alloy-metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x72"
      # App operator info for alloy
      - series: 'app_operator_app_info{name="alloy", namespace="giantswarm", team="team-atlas", installation="golem", cluster_id="golem"}'
        values: "1+0x72"
    alert_rule_test:
      # Alert should not fire before the service is down for 1d
      - alertname: JobScrapingFailureMC
        eval_time: 40h
        exp_alerts: []
      # Alert should fire after being down for 1d (downtime starts at 25h, fires at 49h)
      - alertname: JobScrapingFailureMC
        eval_time: 49h
        exp_alerts:
          - exp_labels:
              area: platform
              severity: notify
              topic: observability
              cancel_if_outside_working_hours: "true"
              app: "prometheus-agent"
              cluster_id: "golem"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "prometheus-agent-metrics"
              team: "team-atlas"
            exp_annotations:
              __dashboardUid__: alloy-metrics-targets
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION=golem&CLUSTER=golem
              summary: "Monitoring agent failed to scrape all targets for an app / job on a management cluster."
              description: "No targets are reachable for prometheus-agent by monitoring on management cluster golem (job: prometheus-agent-metrics)"

  # Test CriticalJobScrapingFailure
  - interval: 1h
    input_series:
      - series: 'up{job="apiserver", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      # critical target up for 5d and down for 5d
      - series: 'up{job="kube-controller-manager", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x120 0+0x120"
      - series: 'up{job="kube-scheduler", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      - series: 'up{job="kubelet", metrics_path="/metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      - series: 'up{job="node-exporter", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
      - series: 'up{job="kube-state-metrics", installation="golem", cluster_id="golem", provider="capa", pipeline="testing"}'
        values: "1+0x240"
    alert_rule_test:
      # Alert should not fire before 3d
      - alertname: CriticalJobScrapingFailure
        eval_time: 30m
        exp_alerts: []
      - alertname: CriticalJobScrapingFailure
        eval_time: 4d
        exp_alerts: []
      # Alert should fire after 3d of downtime for critical job
      - alertname: CriticalJobScrapingFailure
        eval_time: 9d
        exp_alerts:
          - exp_labels:
              area: platform
              severity: page
              team: atlas
              topic: observability
              cluster_id: "golem"
              installation: "golem"
              provider: "capa"
              pipeline: "testing"
              job: "kube-controller-manager"
              cancel_if_outside_working_hours: "true"
            exp_annotations:
              __dashboardUid__: servicemonitors-details
              dashboardQueryParams: "orgId=1"
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/job-scraping-failure/?INSTALLATION=golem&CLUSTER=golem
              summary: "Monitoring agent failed to scrape all targets in a job."
              description: "Monitoring agents for cluster golem/golem has failed to scrape all targets in kube-controller-manager job."


  # Test MetricForwardingErrors
  - interval: 1m
    input_series:
      # Test case where alert should not fire (error rate < 10%)
      - series: 'prometheus_remote_storage_samples_failed_total{remote_name="test-remote", url="http://example.com", installation="foo", cluster_id="bar"}'
        values: '0+0x65 0+50x60' # Minimal failures for 2h+ period
      - series: 'prometheus_remote_storage_samples_total{remote_name="test-remote", url="http://example.com", installation="foo", cluster_id="bar"}'
        values: '1000+1000x125' # Normal sample count

      # Test case where alert should fire (error rate > 10%)
      - series: 'prometheus_remote_storage_samples_failed_total{remote_name="error-remote", url="http://error.com", installation="foo", cluster_id="bar"}'
        values: '0+0x5 0+120x120' # Significant failures over time
      - series: 'prometheus_remote_storage_samples_total{remote_name="error-remote", url="http://error.com", installation="foo", cluster_id="bar"}'
        values: '1000+1000x125' # Normal sample count

    alert_rule_test:
      # Alert shouldn't fire for low error rate
      - eval_time: 65m
        alertname: MetricForwardingErrors
        exp_alerts: []
      
      # Alert shouldn't fire yet (not enough time elapsed)
      - eval_time: 70m
        alertname: MetricForwardingErrors
        exp_alerts: []
      
      # Alert should fire after 1h of high error rate
      - eval_time: 125m
        alertname: MetricForwardingErrors
        exp_alerts:
          - exp_labels:
              alertname: MetricForwardingErrors
              cluster_id: "bar"
              installation: "foo"
              remote_name: error-remote
              url: http://error.com
              area: platform
              cancel_if_outside_working_hours: "true"
              severity: page
              team: atlas
              topic: observability
            exp_annotations:
              summary: Monitoring agent fails to send samples to remote storage.
              description: Monitoring agent failed to send 10.7% of the samples to error-remote:http://error.com.
              runbook_url: https://intranet.giantswarm.io/docs/support-and-ops/runbooks/monitoring-pipeline/?INSTALLATION=foo&CLUSTER=bar
              __dashboardUid__: promRW001
              dashboardQueryParams: "orgId=1"
